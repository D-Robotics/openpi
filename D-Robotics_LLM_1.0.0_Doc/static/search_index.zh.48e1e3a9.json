[{"id":30,"title":"主要接口函数","content":"#\n\n\n创建默认参数#\n\n\n\n创建默认的模型通用参数。\n\n * 返回值：\n   * 构造默认参数的xlm_common_params_t结构体数据。\n\n\n初始化实例#\n\n\n\n初始化实例。\n\n * 参数\n   \n   * [in]：param，初始化生成的模型通用参数。\n   * [in]：callback，注册任务的回调函数指针，即任务的执行实体。\n   * [out]：llm_handle，推理句柄，用于后续任务的管理。\n\n * 返回值\n   \n   * 0（初始化成功），-1（初始化失败）。\n\n\n同步推理#\n\n\n\n同步推理，启动推理包括一次完整的prefil和decode。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n   * [in]：input，模型推理输入，包含prompt、图片、任务优先级等数据。\n   * [in]：userdata，用户自定义数据，随推理结果一同通过回调函数返回。\n\n * 返回值：\n   \n   * 0（正确执行推理任务），-1（获取推理句柄失败，任务返回）。\n\n\nPPL计算#\n\n\n\n仅做ppl计算时使用，正常任务不执行该接口。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n   * [in]：input，模型推理输入，一般是text文本或wikitest数据。\n   * [in]：userdata，用户自定义数据，随推理结果一同通过回调函数返回。\n\n * 返回值：\n   \n   * 0（正确执行PPL计算任务），-1（获取推理句柄失败，任务返回）。\n\n\n异步推理#\n\n\n\n异步处理，启动推理包括一次完整的prefil和decode。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n   * [in]：input，模型推理输入，包含prompt、图片、任务优先级等数据。\n   * [in]：userdata，用户自定义数据，随推理结果一同通过回调函数返回。\n\n * 返回值：\n   \n   * 0（正确执行推理任务），-1（获取推理句柄失败，任务返回）。\n\n\n销毁实例#\n\n\n\n释放推理实例资源。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n\n * 返回值：\n   \n   * 0（正确销毁任务），-1（获取推理句柄失败，接口返回）。\n\n\nOmni音频输入#\n\n\n\nOmni在线运行时，提供音频输入。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n   * [in]：audio_input，音频输入，包含内存首地址和长度信息。\n\n * 返回值：\n   \n   * 0（正确传输音频数据），-1（数据传输失败，任务返回）。\n\n\nOmni视频输入#\n\n\n\nOmni在线运行时，提供视频输入。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n   * [in]：video_input，视频输入，包含y和uv分量首地址和长宽。\n\n * 返回值：\n   \n   * 0（正确传输视频数据），-1（数据传输失败，任务返回）。\n\n\nOmni文本输入#\n\n\n\nOmni在线运行时，提供文本输入。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n   * [in]：text_input，文本输入，包含system文本和user文本。\n\n * 返回值：\n   \n   * 0（正确传输文本数据），-1（数据传输失败，任务返回）。\n\n\nOmni同步推理#\n\n\n\n以同步方式启动omni的全流程处理。\n\n * 参数：\n   \n   * [in]：handle，通过xlm_init接口获取的推理句柄。\n   * [in]：input，模型推理输入。\n   * [in]：userdata，用户自定义数据，随推理结果一同通过回调函数返回。\n\n * 返回值：\n   \n   * 0（正确执行推理任务），-1（获取推理句柄失败，任务返回）。","routePath":"/guide/api/api_details/api_functions","lang":"zh","toc":[{"text":"创建默认参数","id":"创建默认参数","depth":2,"charIndex":3},{"text":"初始化实例","id":"初始化实例","depth":2,"charIndex":76},{"text":"同步推理","id":"同步推理","depth":2,"charIndex":252},{"text":"PPL计算","id":"ppl计算","depth":2,"charIndex":482},{"text":"异步推理","id":"异步推理","depth":2,"charIndex":708},{"text":"销毁实例","id":"销毁实例","depth":2,"charIndex":938},{"text":"Omni音频输入","id":"omni音频输入","depth":2,"charIndex":1056},{"text":"Omni视频输入","id":"omni视频输入","depth":2,"charIndex":1227},{"text":"Omni文本输入","id":"omni文本输入","depth":2,"charIndex":1400},{"text":"Omni同步推理","id":"omni同步推理","depth":2,"charIndex":1575}],"domain":"","frontmatter":{},"version":"latest"},{"id":31,"title":"基本类型定义","content":"#\n\n\n句柄类型#\n\n\n\n * 用于表示推理实例的句柄。","routePath":"/guide/api/api_details/basic_type_definition","lang":"zh","toc":[{"text":"句柄类型","id":"句柄类型","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"latest"},{"id":32,"title":"回调函数类型","content":"#\n\n\n\n * 推理结果回调函数，返回推理结果和状态，在调用初始化接口时注册。","routePath":"/guide/api/api_details/callback","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":33,"title":"结构体与枚举类型","content":"#\n\n\n采样参数#\n\n\n\n * 模型输出采样算法的参数设置。\n\n\n模型类型#\n\n\n\n * 指定模型类型，当前支持DeepSeek、InternLM、Omni、Qwen2.5模型。\n\n\n通用参数#\n\n\n\n * 模型通用参数配置，可通过xlm_create_default_param接口获取默认参数。\n\n\n输入类型#\n\n\n\n * 请求的输入类型。\n\n\n推理后端类型#\n\n\n\n * 推理后端类型设置，支持BPU绑核功能。\n\n\n图片预处理类型#\n\n\n\n * 图像预处理类型设置，目前仅支持动态分辨率。\n\n\nToken输入结构#\n\n\n\n * Token输入类型的结构。\n\n\n图片输入结构#\n\n\n\n * 多模态输入中的图片结构体。\n\n\n多模态输入结构#\n\n\n\n * 多模态输入的结构体。\n   \n   * 支持单prompt单图片。\n   \n   * 支持单prompt多图片。（当前版本暂未支持）\n\n\nOmni视频输入结构#\n\n\n\n * Omni模型的视频在线输入参数结构体。\n\n\nOmni音频输入结构#\n\n\n\n * Omni模型的音频在线输入参数结构体。\n\n\nOmni文本输入结构#\n\n\n\n * Omni模型的文本在线输入参数结构体。\n\n\n优先级类型#\n\n\n\n * 模型输入请求优先级设置。\n   \n   * 抢占关系如下：\n     \n     * XLM_PRIORITY_TYPE_URGENT --抢占--> XLM_PRIORITY_TYPE_HIGH --抢占-->\n       XLM_PRIORITY_TYPE_NORMAL。\n   \n   * 当优先级同为NORMAL时不会发生抢占，但会根据priority的值在决定谁会优先执行。\n     \n     * priority值越高优先级优化，取值范围[0, 253]。\n\n\nPPL参数结构#\n\n\n\n * 模型PPL的参数结构体，当前仅支持DeepSeek模型的PPL。\n\n\n单条推理请求结构#\n\n\n\n * 单条推理请求的结构体。（后续会支持同时输入多条请求）\n\n\n推理输入结构#\n\n\n\n * 模型推理输入的结构体，该结构体作为参数传入推理接口。\n\n\n性能数据结构#\n\n\n\n * 模型性能数据结构体, 在推理结束时会返回本次推理的性能数据\n\n\n推理结果结构#\n\n\n\n * 推理结果返回的结构体。\n\n\n推理状态#\n\n\n\n * 模型当前推理的状态，随推理结果一起返回。","routePath":"/guide/api/api_details/structure_and_enumeration_type","lang":"zh","toc":[{"text":"采样参数","id":"采样参数","depth":2,"charIndex":3},{"text":"模型类型","id":"模型类型","depth":2,"charIndex":32},{"text":"通用参数","id":"通用参数","depth":2,"charIndex":91},{"text":"输入类型","id":"输入类型","depth":2,"charIndex":151},{"text":"推理后端类型","id":"推理后端类型","depth":2,"charIndex":174},{"text":"图片预处理类型","id":"图片预处理类型","depth":2,"charIndex":210},{"text":"Token输入结构","id":"token输入结构","depth":2,"charIndex":249},{"text":"图片输入结构","id":"图片输入结构","depth":2,"charIndex":282},{"text":"多模态输入结构","id":"多模态输入结构","depth":2,"charIndex":312},{"text":"Omni视频输入结构","id":"omni视频输入结构","depth":2,"charIndex":396},{"text":"Omni音频输入结构","id":"omni音频输入结构","depth":2,"charIndex":436},{"text":"Omni文本输入结构","id":"omni文本输入结构","depth":2,"charIndex":476},{"text":"优先级类型","id":"优先级类型","depth":2,"charIndex":516},{"text":"PPL参数结构","id":"ppl参数结构","depth":2,"charIndex":769},{"text":"单条推理请求结构","id":"单条推理请求结构","depth":2,"charIndex":819},{"text":"推理输入结构","id":"推理输入结构","depth":2,"charIndex":864},{"text":"性能数据结构","id":"性能数据结构","depth":2,"charIndex":907},{"text":"推理结果结构","id":"推理结果结构","depth":2,"charIndex":953},{"text":"推理状态","id":"推理状态","depth":2,"charIndex":981}],"domain":"","frontmatter":{},"version":"latest"},{"id":34,"title":"概览","content":"#\n\n通过阅读本章节，您可以了解如何利用API接口对LLM模型进行加载、推理及结果处理。接口调用基本流程图如下所示：\n\n\n\n关于LLM模型推理的API接口，包含基本类型定义、结构体与枚举类型、回调函数类型以及主要接口函数等内容，详细可参见 接口详细说明 章节。","routePath":"/guide/api/api_overview","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":35,"title":"DeepSeek-R1-Distill-Qwen模型性能Benchmark","content":"#\n\n\n测试条件#\n\n * 测试开发板：S100P。\n\n * 性能数据获取：测试单条prompt，取TTFT（首token延迟）和TPS（平均每秒Token数）指标。\n\n * Python版本：Python3.10。\n\n * 运行环境：Linux。\n\n\n实测数据#","routePath":"/guide/benchmark/deepseek_r1_distill_qwen","lang":"zh","toc":[{"text":"测试条件","id":"测试条件","depth":2,"charIndex":3},{"text":"实测数据","id":"实测数据","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":36,"title":"InternLM2模型性能Benchmark","content":"#\n\n\n测试条件#\n\n * 测试开发板：S100P。\n\n * 性能数据获取：测试单条prompt，取TTFT（首token延迟）和TPS（平均每秒Token数）指标。\n\n * Python版本：Python3.10。\n\n * 运行环境：Linux。\n\n\n实测数据#","routePath":"/guide/benchmark/internlm2","lang":"zh","toc":[{"text":"测试条件","id":"测试条件","depth":2,"charIndex":3},{"text":"实测数据","id":"实测数据","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":37,"title":"Qwen2.5模型性能Benchmark","content":"#\n\n\n测试条件#\n\n * 测试开发板：S100P。\n\n * 性能数据获取：测试单条prompt，取TTFT（首token延迟）和TPS（平均每秒Token数）指标。\n\n * Python版本：Python3.10。\n\n * 运行环境：Linux。\n\n\n实测数据#","routePath":"/guide/benchmark/qwen2.5","lang":"zh","toc":[{"text":"测试条件","id":"测试条件","depth":2,"charIndex":3},{"text":"实测数据","id":"实测数据","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":38,"title":"Qwen2.5-Omni模型性能Benchmark","content":"#\n\n\n测试条件#\n\n * 测试开发板：S100P。\n\n * 性能数据获取：测试单条prompt，取TTFT（首token延迟）和TPS（平均每秒Token数）指标。\n\n * Python版本：Python3.10。\n\n * 运行环境：Linux。\n\n\n实测数据#","routePath":"/guide/benchmark/qwen2.5_omni","lang":"zh","toc":[{"text":"测试条件","id":"测试条件","depth":2,"charIndex":3},{"text":"实测数据","id":"实测数据","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":39,"title":"地瓜LLM文档脉络","content":"#\n\n本文档适用于使用地瓜LLM工具链的所有开发者，为您提供全方位的开发过程指导。为您能够充分了解整体的使用过程，地瓜推荐您按照以下顺序进行阅读。\n\n以下为各章节内容简述，您也可以根据需要进行对应章节的阅读。\n\n1. LLM文档脉络\n\n本章节为您准备了整体文档中相关章节的内容简介及内容跳转，以及文档的推荐阅读顺序。\n\n 2. 产品简介\n\n 3. 环境部署\n\n本章节为您介绍开发环境和运行环境下，需要您提前进行的环境部署步骤及内容。\n\n 4. 使用说明\n\n 5. 工具介绍\n\n 6. API介绍\n\n本章节为您介绍地瓜LLM工具链的API接口，帮助您更好地理解和使用API。\n\n 7. 模型性能Benchmark\n\n 8. OpenExplorer-LLM工具链授权使用协议\n\n本章节为我们提供的授权使用协议，在您使用地瓜LLM工具链前请务必仔细阅读。","routePath":"/guide/doc_introduction","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":40,"title":"环境部署","content":"#\n\n本章节向您介绍如何在x86开发机和S100/S100P端侧完成环境部署。\n\n\nx86量化环境部署#\n\n\n推荐硬件配置#\n\nCPU: Intel(R) Core(TM) i9-14900K 32核\n\nGPU: NVIDIA RTX 3090\n\nDDR: 128G\n\n注意\n\n请注意，若硬件配置较低，可能导致量化速度变慢或失败。\n\n\n部署包准备#\n\n下载我们提供的 D-Robotics_LLM_{version}.tar.gz 安装包并正确解压到x86开发机。\n\n\n安装Conda环境#\n\n注解\n\n如已安装Conda环境，请跳过此步骤。\n\n为了方便进行Python环境的管理，我们推荐使用Miniforge3作为Python的包管理工具。\n\n 1. 下载Miniforge3安装包：\n    \n    \n\n 2. 安装Miniforge3：\n    \n    \n\n\n设置Conda环境#\n\n 1. 进入Conda base环境中，参考如下命令：\n    \n    \n\n 2. 创建并进入python3.10的环境，参考如下：\n    \n    \n    \n    进入oellm conda环境：\n    \n    \n    \n    安装必要的python环境：\n    \n    \n\n\n安装部署包内其他文件#\n\nConda环境设置完成后，需要继续安装LLM部署包内的模型量化转换和编译工具包，参考如下命令：\n\n\n\n至此，我们完成了x86开发机模型量化编译环境的部署，如果您有交叉编译C++端侧可执行文件的需求，可继续执行以下步骤。\n\n\n交叉编译工具链配置#\n\n在LLM部署包内我们提供了交叉编译工具链的安装包，用于在x86平台交叉编译arm程序。您可在x86创建目录并解压安装包，参考如下命令：\n\n注意\n\n请注意，安装至/opt目录需要root权限，若无root权限，您也可以选择解压至其他目录。\n\n\n\n解压后，配置交叉编译工具链的环境变量，参考如下命令：\n\n\n\n至此，您可以在x86平台交叉编译arm可执行程序，oellm_runtime的每个example都提供了build脚本供您使用。\n\n\n开发板运行环境部署#\n\n请参考内存分配说明完成开发板内存分配操作。","routePath":"/guide/env_install","lang":"zh","toc":[{"text":"x86量化环境部署","id":"x86量化环境部署","depth":2,"charIndex":41},{"text":"推荐硬件配置","id":"推荐硬件配置","depth":3,"charIndex":54},{"text":"部署包准备","id":"部署包准备","depth":3,"charIndex":167},{"text":"安装Conda环境","id":"安装conda环境","depth":3,"charIndex":234},{"text":"设置Conda环境","id":"设置conda环境","depth":3,"charIndex":384},{"text":"安装部署包内其他文件","id":"安装部署包内其他文件","depth":3,"charIndex":546},{"text":"交叉编译工具链配置","id":"交叉编译工具链配置","depth":3,"charIndex":671},{"text":"开发板运行环境部署","id":"开发板运行环境部署","depth":2,"charIndex":902}],"domain":"","frontmatter":{},"version":"latest"},{"id":41,"title":"地瓜LLM工具链授权使用协议","content":"#\n\n注意\n\n重要通知 - 请您在使用该工具链之前务必仔细阅读以下内容。\n\n如果您未年满签订本协议所需达到的年龄或未获得所需授权，或者您不接受以下所有条款，请勿使用该工具链。如您选择使用工具链，意味着您充分知悉、理解并接受以下内容。\n\n本授权使用协议（以下简称“本协议”）是您（无论是个人还是企业或组织等实体都简称“您”）与地瓜之间的法律协议，\n旨在约束您对地瓜所提供的LLM工具链软件包及相关文档资料（简称“工具链”）的使用。\n\n 1.  定义\n     \n     1.1. “知识产权”指任何国家/地区的法律规定的知识产权和其他无形权利，包括但不限于：\n     专利（发明专利、实用新型专利和外观设计专利等）、著作权（包括财产权利和精神权利）、商标、商品名称、包装、装潢、数据、数据库、商业秘密、保密信息、技术诀\n     窍、专有技术等。而且每种情况下，无论权利已注册或未注册；包括进行申请、被授予、续展或延期、要求优先权的权利；以及所有现在和未来的类似或等效的权利。\n     \n     1.2. “关联方”指任何直接或间接地控制一方、被一方控制或与一方共同受控于同一控制人的任何实体。\n     “控制”，包括“被控制”“共同受控”等相关词语，指当前或此后、直接或间接，通过表决权、协议安排或者其他方式，拥有决定受控实体管理层和政策方向的权力；\n     而且，在不影响前述意思的前提下，只要任何实体持有或控制了任何受控实体已发行在外的有表决权之证券或者其他形式之所有者权益至少50%的表决权和/或代理表决\n     权，则会被视为对该受控实体拥有前述控制权力。\n\n 2.  许可\n     \n     2.1.\n     在您遵守本协议约定的前提下，地瓜同意授予您一项有限的、非排他、不可转让、不可转许可、不可分许可、可随时撤销的许可，使得您可以安装、使用该工具链的副本。\n     \n     2.2.\n     地瓜保留所有未明确授予您的权利。除本协议第2.1条明确授予您的许可外，本协议的任何内容都不应被理解为以明示、默示，或任何其他方式向您许可或转让任何所有\n     权、知识产权或其他权益。\n\n 3.  使用限制\n     \n     以下是针对工具链的使用限制。如您未遵守这些限制，您对因此引起的后果自行负责：\n     \n     3.1. 您不得对工具链进行任何形式的修改；\n     \n     3.2. 您不得对工具链进行任何形式的反向工程、反汇编、反编译等以获悉工具链的任何源代码、内部结构或设计；\n     \n     3.3. 除本协议明确授权外，您不得出售、出租、再许可、分发、转让或以任何形式向第三方提供或披露该工具链；\n     \n     3.4. 在使用工具链时，您不得使得工具链或其任何部分受限于任何开源软件许可协议；\n     \n     3.5. 您不得从工具链上去除任何版权或其他专有权利的声明或标签；\n     \n     3.6. 您不得利用工具链去开发或协助第三方开发与工具链相竞争的产品或技术；\n     \n     3.7. 您不得将工具链在任一国家或地区申请、注册、登记任何形式的知识产权，否则您应按照地瓜要求立即将该等知识产权免费转让给地瓜或其指定的第三方；\n     \n     3.8.\n     您不得利用工具链用以支持任何针对地瓜、地瓜的关联方、地瓜的供应商，及其各自客户的知识产权侵权或不正当竞争诉讼或类似主张，也不得对地瓜、地瓜的关联方、地\n     瓜的供应商，及其各自客户的已注册或已申请的知识产权（包括但不限于已授权专利、申请中专利）提出可能破坏或影响该等知识产权效力的申请。\n\n 4.  免责声明\n     \n     4.1. 该工具链由地瓜按原样（“AS\n     IS”）提供，因此地瓜不对工具链的非侵权、适销性、特定用途适用性、兼容性、正确性、准确性、完整性、可靠性、性能、无瑕疵做任何形式（明示或默示）的声明或\n     保证，也不会就任何已知或未知的错误或缺陷承担任何质量责任，包括但不限于做出通知、改正、修改、发布升级补丁。\n     在不限制前述规定的情况下，地瓜不保证：该工具链将能满足您的要求；任何缺陷或错误都将得到纠正；任何特定内容都将可用；或者该工具链不含病毒或其他有害组件。\n     \n     4.2. 在任何情况下，地瓜不对您因使用或无法使用工具链的部分或全部而遭受的任何直接性、间接性、后果性、附带性或惩罚性的损失损害承担任何责任，\n     包括但不限于收入损失、利润损失、数据灭失或商业中断导致的损失、采购替代商品或服务的费用、有关知识产权侵权或不正当竞争等第三方索赔，即使您已事先告知该等\n     损失损害发生的可能性。\n     \n     4.3.\n     您同意保护地瓜及其关联方、雇员、客户、合作伙伴等相关人员免遭任何由于您使用工具链引起的任何索赔、责任、损害、损失或费用，包括但不限于第三方索赔、法律费\n     用。\n\n 5.  反馈\n     \n     您可以提供与该工具链有关的建议、请求、修改、完善或其他反馈（统称为“反馈”），但并无义务这么做。如果您提供反馈，则该反馈相关的知识产权均归属于地瓜。\n\n 6.  终止\n     \n     6.1.\n     如果您未能遵守本协议中的任何条款，或者您基于该工具链启动或参与了针对地瓜、地瓜关联方及/或地瓜客户的任何法律诉讼、仲裁，则地瓜有权立即单方终止本协议。\n     一旦本协议终止，您必须停止使用并销毁您拥有的所有工具链相关副本。\n     \n     6.2. 您可以随时通过停止使用该工具链并销毁您拥有的所有相关工具链副本来终止本协议。\n     \n     6.3. 除本协议第2条“许可”外，本协议终止后，本协议所有其他条款均继续有效。\n\n 7.  转让\n     \n     未经地瓜事先书面同意，您不得转让或转移您在本协议中的任何权利或义务，对此类行为的任何尝试均属无效。\n\n 8.  贸易合规\n     \n     8.1.\n     您同意并承诺：（i）严格遵守中国和包括欧盟各国、美国在内的所有适用的进出口管制相关的法律法规（统称“出口管制法律法规”，包括但不限于美国《出口管制条例\n     》（EAR）等）； 及（ii）不违反出口管制法律法规关于物项、目的地、最终用户、最终用途的限制性规定。\n     \n     8.2.\n     如果根据监管机构或地瓜要求需要进行出口管制合规检查，您在收到要求后，应及时根据检查要求提供工具链有关的最终用户、目的地和用途等信息，配合合规检查。\n     \n     8.3.\n     尽管本协议有任何其他规定，如您违反出口管制法律法规，或您及/或您的关联方被列入相关政府禁止参与出口活动的任何个人或实体名单（统称“制裁名单”），地瓜有\n     权立即单方终止本协议。 一旦本协议终止，您必须停止使用并销毁您拥有的所有工具链相关副本。\n\n 9.  适用法律和管辖权\n     \n     9.1. 本协议的订立、效力、解释、履行、争议的解决等适用中华人民共和国法律法规的有关规定，且排除法律冲突原则及《联合国国际货物销售合同公约》的适用。\n     \n     9.2.\n     双方同意如有任何争议，应彼此友好协商解决。如协商不能解决，任一方可以提请北京仲裁委员会（“BAC”）按照该会现行仲裁规则进行仲裁。仲裁地在北京，仲裁裁\n     决是终局的，对您和地瓜均有约束力。\n\n 10. 其他\n     \n     10.1. 关于该工具链的授权使用事宜，双方同意本协议构成双方之间的完整且排他的协议，并取代双方之间就该事宜做出的任何口头或书面沟通、承诺。\n     任何附加或不同的条款或条件对双方均不具有约束力且无效。本协议只能通过加盖双方合同章或公章的书面形式进行修改。\n     \n     10.2. 本协议有任何条款因任何原因不可执行、不合法或无效，概不影响本协议其他条款。\n     \n     10.3. 本协议有中文和英文两个版本。如中英文意思不一致的，以中文为准。","routePath":"/guide/license_agreement","lang":"zh","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":42,"title":"开发流程","content":"#\n\n地瓜LLM的整体开发步骤主要包括环境部署、大模型量化以及端侧部署。\n\n\n环境部署#\n\n这一阶段中，您需要按照我们的要求完成开发环境以及运行环境(端侧)的正确安装部署，以便进行后续的流程，详细介绍可参考环境部署 章节的介绍。\n\n\n大模型量化#\n\n这一阶段中，需要您在HuggingFace下载您所需的大语言模型，我们会将这个模型转换为能够在地瓜平台上部署的格式， 以便进行下一步的推理，包括如下步骤：\n\n 1. 获取原始模型，我们支持的模型类型包括：\n    \n    * 开源的Hugging Face格式的\n      DeepSeek-R1-Distill-Qwen-1.5B、DeepSeek-R1-Distill-Qwen-7B、InternLM2-1.8B、Q\n      wen2.5-1.5B、Qwen2.5-7B、Qwen2.5-1.5B-Instruct、Qwen2.5-7B-Instruct、Qwen2.5-O\n      mni-3B 模型。\n    \n    * 自行训练得到的大语言模型，要求与上方提到的Hugging Face格式的模型结构一致。\n\n 2. 模型量化(也可直接使用我们提供的转换好的模型跳过此步)：\n    \n    通过 oellm_build 命令行工具进行转换编译，构建能够部署在地瓜平台上部署的模型，在构建过程中可根据实际使用情况对量化参数进行设置。 得到的\n    *.hbm 文件用于后续的端侧部署。\n\n\n端侧部署#\n\n这一阶段中，可以进行上一阶段我们得到的量化后模型的实际部署和推理运行，包括如下步骤：\n\n 1. 交叉编译工具链配置，此过程需要正确解压交叉编译工具链安装包并配置路径。\n\n 2. 通过预提供的编译脚本生成用于端侧部署的可执行文件。\n\n 3. 端侧推理运行，准备好全部用于端侧部署的模型、可执行文件以及依赖文件之后，在端侧运行可执行文件，运行完成后即可进行端侧的推理运行，此时可进行对话测试。\n\n 4. 资源释放，在完成运行流程后，销毁推理任务接口，释放占用的资源。\n\n环境部署、模型量化以及端侧部署三个步骤共同构成了完整的D-Robotics-LLM的开发流程，确保大语言模型能够成功完成转换、调优，最终在地瓜平台上实现高效部署\n。","routePath":"/guide/preface/develop_process","lang":"zh","toc":[{"text":"环境部署","id":"环境部署","depth":2,"charIndex":38},{"text":"大模型量化","id":"大模型量化","depth":2,"charIndex":116},{"text":"端侧部署","id":"端侧部署","depth":2,"charIndex":634}],"domain":"","frontmatter":{},"version":"latest"},{"id":43,"title":"产品简介","content":"#\n\n\n什么是地瓜LLM#\n\n地瓜LLM工具链（也称D-Robotics-LLM）是地瓜S100工具链产品面向大模型业务的特性化拓展，\n可以实现在S100系列平台及衍生平台上进行大语言模型的开发、转换和部署。基于此开发的应用参考解决方案，可为机器人业务中的大模型业务诉求提供案例支撑。\n在您获取到地瓜LLM工具链发布包后，可以先按照如下步骤进行了解。\n\n 1. 先参考 发布物内容 小节，了解发布包的目录结构。\n\n 2. 再参考 开发流程 章节，简单了解整体部署包的使用流程。\n\n 3. 接着参考 环境部署 章节，正确完成所需环境的安装部署。\n\n 4. 最后根据您所选择的平台及使用的模型，参考 S100/S100P平台 章节，完成模型转换与部署的全流程。\n\n更多更全面的地瓜LLM相关使用教程，欢迎参考后方章节进行了解。\n\n\n发布物内容 #\n\n\nS100/S100P#\n\n地瓜LLM发布包的结构如下所示：\n\n\n\n注解\n\n此处仅为展示发布包结构，具体版本号请以实际您使用的发布包为准。\n\n其中：\n\n * arm-gnu-toolchain-11.3.rel1-x86_64-aarch64-none-linux-gnu.tar.xz为发布包依赖的交叉编译工具\n   包。\n\n * oellm_build文件夹下放置环境、编译器以及leap统一算子库相关依赖。\n   \n   * requirements.txt为搭建开发环境所需的环境配置文件。\n   \n   * hbdk4_compiler-{version}-cp310-cp310-manylinux_2_17_x86_64.whl为编译器hbdk4的安装包\n     。\n   \n   * hbdk4_runtime_aarch64_unknown_linux_gnu_nash-{version}-py3-none-any.whl为编译器\n     hbdk4运行时的安装包。\n   \n   * leap_llm-{version}-py310-none-any.whl为leap-llm安装包，提供模型的量化和转换功能。\n\n * oellm_runtime文件夹下放置端侧部署SDK相关依赖及示例等内容。\n   \n   * model 下提供编译好的板端模型的下载链接，您可将模型置于此文件夹。\n   \n   * config 下包含预置的已支持模型的参考配置文件。\n   \n   * example 下包含demo源码，编译脚本，二进制文件，运行时的配置文件等。\n   \n   * include 下包含编译端侧部署示例所依赖的关键头文件。\n   \n   * lib 下包含端侧部署程序依赖库。\n   \n   * set_performance_mode.sh 用于修改硬件寄存器的值使设备调整为性能模式，当前仅支持调整S100P。","routePath":"/guide/preface/preface","lang":"zh","toc":[{"text":"什么是地瓜LLM","id":"什么是地瓜llm","depth":2,"charIndex":3},{"text":"发布物内容","id":"发布物内容","depth":2,"charIndex":-1},{"text":"S100/S100P","id":"s100s100p","depth":3,"charIndex":373}],"domain":"","frontmatter":{},"version":"latest"},{"id":44,"title":"S100/S100P平台","content":"#\n\n\n概述#\n\n本章节将为您介绍基于S100/S100P平台的D-Robotics-LLM的使用流程。为您提供内存分配说明，以及DeepSeek-R1-Distill-Qwen\n，InternLM2，Qwen2.5和Qwen2.5-Omni模型的使用流程介绍，帮助您对其全过程的使用进行充分了解。\n\n * 内存分配说明：本章节将为您介绍端侧的内存分配，您可以在S100/S100P平台使用不同的内存分配策略以满足不同模型的运行需求。\n\n * DeepSeek-R1-Distill-Qwen模型开发：本章节将为您介绍基于DeepSeek-R1-Distill-Qwen模型的D-Robotics-\n   LLM的全流程操作体系。将按简易开发与进阶开发两部分，分别为您阐述不同技术场景下的使用流程。\n\n * InternLM2模型开发：本章节将为您介绍基于InternLM2模型的D-Robotics-LLM的全流程操作体系。将按简易开发与进阶开发两部分，分别为\n   您阐述不同技术场景下的使用流程。\n\n * Qwen2.5模型开发：本章节将为您介绍基于Qwen2.5模型的D-Robotics-LLM的全流程操作体系。将按简易开发与进阶开发两部分，分别为您阐述不\n   同技术场景下的使用流程。\n\n * Qwen2.5-Omni模型开发：本章节将为您介绍基于Qwen2.5-Omni模型的D-Robotics-LLM的全流程操作体系。将按简易开发与进阶开发两\n   部分，分别为您阐述不同技术场景下的使用流程。\n\n\n支持模型#\n\n在S100/S100P平台上，D-Robotics-LLM目前支持以下模型：\n\n * DeepSeek-R1-Distill-Qwen模型：DeepSeek-R1-Distill-Qwen-1.5B和DeepSeek-R1-Distill-\n   Qwen-7B。\n\n * InternLM2模型：InternLM2-1.8B。\n\n * Qwen2.5模型：Qwen2.5-1.5B、Qwen2.5-7B、Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct。\n\n * Qwen2.5-Omni模型：Qwen2.5-Omni-3B。","routePath":"/guide/quickstart/S100P","lang":"zh","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":3},{"text":"支持模型","id":"支持模型","depth":2,"charIndex":659}],"domain":"","frontmatter":{},"version":"latest"},{"id":45,"title":"DeepSeek-R1-Distill-Qwen模型开发","content":"#\n\n\n概述#\n\n本章节将为您介绍基于DeepSeek-R1-Distill-Qwen模型的D-Robotics-LLM的全流程操作体系。\n这里将从简易开发与进阶开发两部分进行展开，覆盖不同技术场景下的使用流程，可根据您的实际使用场景选择。\n\n * 简易开发：本章节介绍了以DeepSeek-R1-Distill-Qwen-1.5B模型为例的简易开发流程，涵盖从模型和部署包准备到板端运行的关键步骤，帮助\n   您快速验证DeepSeek-R1-Distill-Qwen模型开发。\n\n * 进阶开发：本章节基于DeepSeek-R1-Distill-Qwen模型的全周期开发体系，覆盖了从模型和部署包准备、模型量化、端侧部署、板端运行准备到板端\n   运行的全流程开发链路，帮助您进一步了解模型部署的全过程。\n\n\n支持模型#\n\nD-Robotics-LLM为您提供DeepSeek-R1-Distill-Qwen模型量化和端侧部署功能。当前所支持的模型包括DeepSeek-R1-Dist\nill-Qwen-1.5B和DeepSeek-R1-Distill-Qwen-7B。","routePath":"/guide/quickstart/S100P/deepseek_r1_distill_qwen","lang":"zh","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":3},{"text":"支持模型","id":"支持模型","depth":2,"charIndex":356}],"domain":"","frontmatter":{},"version":"latest"},{"id":46,"title":"进阶开发","content":"#\n\n在本章节，我们将为您介绍D-Robotics-LLM的进阶开发使用流程。\n\n此流程适用场景如下：\n\n 1. 已对模型进行微调（finetune），且需重新量化的场景。\n\n 2. 简单的单轮对话。\n\n 3. 连续多轮对话，模型可记忆前几轮的提问和回答。\n\n 4. 在端侧统计模型的PPL数值。\n\n针对以上几种场景，这里我们还是以DeepSeek-R1-Distill-Qwen-1.5B模型为例，为您进行使用说明。\n\n\n环境准备#\n\n请确保已按照 环境部署 章节正确完成开发机及开发板的环境准备。\n\n\n部署包准备#\n\n下载我们提供的D-Robotics_LLM_{version}.tar.gz部署包并解压。\n\n\n模型准备#\n\n注意\n\n当前仅支持DeepSeek-R1-Distill-Qwen-1.5B模型、DeepSeek-R1-Distill-Qwen-7B模型。\n在下载模型前，请确保您已了解模型的使用许可、依赖环境等必要信息，以保证模型后续能正常使用。\n\n您可以通过Hugging Face平台获取DeepSeek系列的模型，以下为模型的下载链接：\n\n * DeepSeek-R1-Distill-Qwen-1.5B模型\n * DeepSeek-R1-Distill-Qwen-7B模型\n\n\n模型量化 #\n\nD-Robotics-LLM为您提供了使用命令行量化编译生成板端模型的功能，此时我们以DeepSeek-R1-Distill-Qwen-1.5B模型为例进行说明\n，参考命令如下：\n\n\n\n注解\n\n关于oellm_build工具的详细使用方法及注意事项，请参考 oellm_build工具 章节。\n\n若您通过resolve_model.txt提供的链接来获取我们编译好的hbm模型，则可跳过此模型量化步骤。\n\nresolve_model.txt文件中提供的DeepSeek模型，均以chunk_size配置为256编译生成，且cache_len参数分别使用1024和40\n96两种配置，其差异已在模型文件名中体现。\n\n\n板端运行准备#\n\n在D-Robotics_LLM_{version}/oellm_runtime/example目录中，我们在各子目录提前准备好了编译后的可执行文件，可以直接在板\n端运行。您也可以执行不同的编译脚本，自行生成所需文件，参考命令如下：\n\n\n\n接下来在板端创建工作目录，参考命令如下：\n\n\n\n在上板之前，您需要确保已经准备如下内容：\n\n * 可运行的开发板，用于实际执行板端程序运行。\n\n * 一个可上板运行的模型（*.hbm），即 模型量化 的产出物。\n\n * 可执行文件（oellm_run，oellm_multichat和oellm_ppl文件）。\n\n * 板端程序依赖库，为了降低部署成本，您可以直接使用D-Robotics-LLM包内D-Robotics_LLM_{version}/oellm_runtim\n   e/set_performance_mode.sh、\n   D-Robotics_LLM_{version}/oellm_runtime/lib文件夹、D-Robotics_LLM_{version}/oellm_\n   runtime/config文件夹和D-Robotics_LLM_{version}/oellm_runtime/example文件夹中的内容。\n\n准备好之后，我们将模型文件（*.hbm）、可执行文件及依赖库整合到一起，参考目录结构如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行#\n\n\n简单会话#\n\n板端运行参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数                参数说明                            是否可选\n-h, --help        显示帮助信息。                         /\n--hbm_path        用于指定量化后的模型文件（*.hbm）路径。          必填\n--tokenizer_dir   用于指定分词器配置路径。                    必填\n--template_path   用于指定对话模板路径。                     必填\n--model_type      用于指定运行的模型类型，当前DeepSeek模型类型为1。   必填\n\n\n多轮对话#\n\n板端运行参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数             参数说明              是否可选\n-h, --help     显示帮助信息。           /\n-c, --config   用于指定json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数              参数说明                            是否可选\nhbm_path        用于指定量化后的模型文件（*.hbm）路径。          必填\ntokenizer_dir   用于指定分词器配置路径。                    必填\ntemplate_path   用于指定对话模板路径。                     必填\nmodel_type      用于指定运行的模型类型，当前DeepSeek模型类型为1。   必填\nbpu_core        用于指定使用的BPU核。默认值为-1，任意核。         可选\n\n\nPPL评估#\n\n板端统计模型的PPL数值参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数             参数说明              是否可选\n-h, --help     显示帮助信息。           /\n-c, --config   用于指定json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数              参数说明                                               是否可选\nhbm_path        用于指定量化后的模型文件（*.hbm）路径。                             必填\ntokenizer_dir   用于指定分词器配置路径。                                       必填\nmodel_type      用于指定运行的模型类型，当前DeepSeek模型类型为1。                      必填\nppl_testcase    用于指定测试文件路径，当前仅支持bin格式。                             必填\nmax_length      用于指定每次送入模型的序列长度。                                   必填\nstride          用于指定测试步长。                                          必填\nbpu_core        用于指定使用的BPU核。默认值为-1，任意核。                            可选\nload_ckpt       是否读取上次测试中断后的进度信息继续测试，默认值为false。                    可选\ntext_data_num   用于指定截断文本到特定长度再测试，若text_data_num <= 0，则不截断，默认值为0。   可选\n\n\n运行结果#\n\n\n简单会话#\n\n简单对话测试参考如下：\n\n\n\n\n多轮对话#\n\n多轮对话测试参考如下：\n\n\n\n\nPPL评估#\n\nPPL评估完成后，会在同目录生成{ppl_testcase}.json文件，其中Perplexity所对应的数值，即为最终的PPL测试结果。参考如下：\n\n\n\n注解\n\n 1. PPL支持断点续测，程序运行时会在执行目录生成ppl_ckpt.json文件，当load_ckpt为true时会读取该文件，从中断处继续测试。\n\n 2. PPL程序执行结束后，会在bin测试文件所在目录生成json文件，包含本次测试的关键参数和PPL计算结果。\n\n 3. ppl_testcase参数设定的bin文件可从parquet转换得到，提供convert_parquet_to_bin.py示例参考代码如下：\n\n","routePath":"/guide/quickstart/S100P/deepseek_r1_distill_qwen/advanced_development_deepseek","lang":"zh","toc":[{"text":"环境准备","id":"环境准备","depth":2,"charIndex":211},{"text":"部署包准备","id":"部署包准备","depth":2,"charIndex":252},{"text":"模型准备","id":"模型准备","depth":2,"charIndex":308},{"text":"模型量化","id":"模型量化","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":868},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":1598},{"text":"简单会话","id":"简单会话","depth":3,"charIndex":1606},{"text":"多轮对话","id":"多轮对话","depth":3,"charIndex":1961},{"text":"PPL评估","id":"ppl评估","depth":3,"charIndex":2449},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":3340},{"text":"简单会话","id":"简单会话-1","depth":3,"charIndex":3348},{"text":"多轮对话","id":"多轮对话-1","depth":3,"charIndex":3371},{"text":"PPL评估","id":"ppl评估-1","depth":3,"charIndex":3394}],"domain":"","frontmatter":{},"version":"latest"},{"id":47,"title":"简易开发","content":"#\n\n本章节中，我们将为您介绍D-Robotics-LLM的基本使用流程，便于您实现快速上手。这里我们以DeepSeek-R1-Distill-Qwen-1.5B模型\n为例，为您进行使用说明。\n\n\n模型及部署包准备#\n\n * 下载我们提供的D-Robotics_LLM_.tar.gz部署包并解压。\n\n * 下载我们提供的DeepSeek_R1_Distill_Qwen_1.5B_1024.hbm模型。\n\n提示\n\n关于hbm模型的下载链接，请参见oellm_runtime中model文件夹的resolve_model.txt文件。\n\n当部署包以及模型准备完毕后，我们将模型（*.hbm）和部署包中的oellm_runtime SDK整合到一起，参考目录结构如下：\n\n\n\n\n板端运行准备#\n\n在板端创建工作目录，参考命令如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行#\n\n模型板端运行参考指令如下：\n\n\n\n其中，运行参数如下：\n\n参数                参数说明                            是否可选\n-h, --help        显示帮助信息。                         /\n--hbm_path        用于指定量化后的模型文件（*.hbm）路径。          必填\n--tokenizer_dir   用于指定分词器配置路径。                    必填\n--template_path   用于指定对话模板路径。                     必填\n--model_type      用于指定运行的模型类型，当前DeepSeek模型类型为1。   必填\n\n\n运行结果#\n\n简单对话测试参考如下：\n\n","routePath":"/guide/quickstart/S100P/deepseek_r1_distill_qwen/simple_development_deepseek","lang":"zh","toc":[{"text":"模型及部署包准备","id":"模型及部署包准备","depth":2,"charIndex":98},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":337},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":494},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":851}],"domain":"","frontmatter":{},"version":"latest"},{"id":48,"title":"InternLM2模型开发","content":"#\n\n\n概述#\n\n本章节将为您介绍基于InternLM2模型的D-Robotics-LLM的全流程操作体系。\n这里将从简易开发与进阶开发两部分进行展开，覆盖不同技术场景下的使用流程，可根据您的实际使用场景选择。\n\n * 简易开发：本章节介绍了以InternLM2-1.8B模型为例的简易开发流程，涵盖从模型和部署包准备到板端运行的关键步骤，帮助您快速验证InternLM2模\n   型开发。\n\n * 进阶开发：本章节基于InternLM2模型的全周期开发体系，覆盖了从模型和部署包准备、模型量化、端侧部署、板端运行准备到板端运行的全流程开发链路，帮助您进\n   一步了解模型部署的全过程。\n\n\n支持模型#\n\nD-Robotics-LLM为您提供InternLM2模型量化和端侧部署功能。当前所支持的模型包括InternLM2-1.8B。","routePath":"/guide/quickstart/S100P/internlm2","lang":"zh","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":3},{"text":"支持模型","id":"支持模型","depth":2,"charIndex":296}],"domain":"","frontmatter":{},"version":"latest"},{"id":49,"title":"进阶开发","content":"#\n\n在本章节，我们将为您介绍D-Robotics-LLM的进阶开发使用流程。\n\n此流程适用场景如下：\n\n 1. 已对模型进行微调（finetune），且需重新量化的场景。\n\n 2. 简单的单轮对话。\n\n 3. 在端侧统计模型的PPL数值。\n\n针对以上几种场景，这里我们还是以InternLM2-1.8B模型为例，为您进行使用说明。\n\n\n环境准备#\n\n请确保已按照 环境部署 章节正确完成开发机及开发板的环境准备。\n\n\n部署包准备#\n\n下载我们提供的D-Robotics_LLM_{version}.tar.gz部署包并解压。\n\n\n模型准备#\n\n注意\n\n当前仅支持InternLM2-1.8B模型。 在下载模型前，请确保您已了解模型的使用许可、依赖环境等必要信息，以保证模型后续能正常使用。\n\n您可以通过Hugging Face平台获取InternLM2系列的模型，以下为模型的下载链接：\n\n * InternLM2-1.8B模型\n\n\n模型量化 #\n\nD-Robotics-LLM为您提供了使用命令行量化编译生成板端模型的功能，此时我们以InternLM2-1.8B模型为例进行说明，参考命令如下：\n\n\n\n注解\n\n关于oellm_build工具的详细使用方法及注意事项，请参考 oellm_build工具 章节。\n\n若您通过resolve_model.txt提供的链接来获取我们编译好的hbm模型，则可跳过此模型量化步骤。\n\nresolve_model.txt文件中提供的InternLM2模型，以chunk_size配置为256编译生成，且cache_len参数使用1024。\n\n\n板端运行准备#\n\n在D-Robotics_LLM_{version}/oellm_runtime/example目录中，我们在各子目录提前准备好了编译后的可执行文件，可以直接在板\n端运行。您也可以执行不同的编译脚本，自行生成所需文件，参考命令如下：\n\n\n\n接下来在板端创建工作目录，参考命令如下：\n\n\n\n在上板之前，您需要确保已经准备如下内容：\n\n * 可运行的开发板，用于实际执行板端程序运行。\n\n * 一个可上板运行的模型（*.hbm），即 模型量化 的产出物。\n\n * 可执行文件（oellm_run和oellm_ppl文件）。\n\n * 板端程序依赖库，为了降低部署成本，您可以直接使用D-Robotics-LLM包内D-Robotics_LLM_{version}/oellm_runtim\n   e/set_performance_mode.sh、\n   D-Robotics_LLM_{version}/oellm_runtime/lib文件夹、D-Robotics_LLM_{version}/oellm_\n   runtime/config文件夹和D-Robotics_LLM_{version}/oellm_runtime/example文件夹中的内容。\n\n准备好之后，我们将模型文件（*.hbm）、可执行文件及依赖库整合到一起，参考目录结构如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行#\n\n\n简单会话#\n\n板端运行参考指令如下：\n\n\n\n其中，运行参数如下：\n\n参数                参数说明                            是否可选\n-h, --help        显示帮助信息。                         /\n--hbm_path        用于指定量化后的模型文件（*.hbm）路径。          必填\n--tokenizer_dir   用于指定分词器配置路径。                    必填\n--model_type      用于指定运行的模型类型，当前InternLM模型类型为4。   必填\n\n\nPPL评估#\n\n板端统计模型的PPL数值参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数             参数说明              是否可选\n-h, --help     显示帮助信息。           /\n-c, --config   用于指定json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数              参数说明                                               是否可选\nhbm_path        用于指定量化后的模型文件（*.hbm）路径。                             必填\ntokenizer_dir   用于指定分词器配置路径。                                       必填\nmodel_type      用于指定运行的模型类型，当前InternLM2模型类型为4。                     必填\nppl_testcase    用于指定测试文件路径，当前仅支持bin格式。                             必填\nmax_length      用于指定每次送入模型的序列长度。                                   必填\nstride          用于指定测试步长。                                          必填\nbpu_core        用于指定使用的BPU核。默认值为-1，任意核。                            可选\nload_ckpt       是否读取上次测试中断后的进度信息继续测试，默认值为false。                    可选\ntext_data_num   用于指定截断文本到特定长度再测试，若text_data_num <= 0，则不截断，默认值为0。   可选\n\n\n运行结果#\n\n\n简单会话#\n\n简单对话测试参考如下：\n\n\n\n\nPPL评估#\n\nPPL评估完成后，会在同目录生成{ppl_testcase}.json文件，其中Perplexity所对应的数值，即为最终的PPL测试结果。参考如下：\n\n\n\n注解\n\n 1. PPL支持断点续测，程序运行时会在执行目录生成ppl_ckpt.json文件，当load_ckpt为true时会读取该文件，从中断处继续测试。\n\n 2. PPL程序执行结束后，会在bin测试文件所在目录生成json文件，包含本次测试的关键参数和PPL计算结果。\n\n 3. ppl_testcase参数设定的bin文件可从parquet转换得到，提供convert_parquet_to_bin.py示例参考代码如下：\n\n","routePath":"/guide/quickstart/S100P/internlm2/advanced_development_internlm2","lang":"zh","toc":[{"text":"环境准备","id":"环境准备","depth":2,"charIndex":168},{"text":"部署包准备","id":"部署包准备","depth":2,"charIndex":209},{"text":"模型准备","id":"模型准备","depth":2,"charIndex":265},{"text":"模型量化","id":"模型量化","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":691},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":1405},{"text":"简单会话","id":"简单会话","depth":3,"charIndex":1413},{"text":"PPL评估","id":"ppl评估","depth":3,"charIndex":1715},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":2606},{"text":"简单会话","id":"简单会话-1","depth":3,"charIndex":2614},{"text":"PPL评估","id":"ppl评估-1","depth":3,"charIndex":2637}],"domain":"","frontmatter":{},"version":"latest"},{"id":50,"title":"简易开发","content":"#\n\n本章节中，我们将为您介绍D-Robotics-LLM的基本使用流程，便于您实现快速上手。这里我们以InternLM2-1.8B模型为例，为您进行使用说明。\n\n\n模型及部署包准备#\n\n * 下载我们提供的D-Robotics_LLM_.tar.gz部署包并解压。\n\n * 下载我们提供的InternLM2_1.8B_1024.hbm模型。\n\n提示\n\n关于hbm模型的下载链接，请参见oellm_runtime中model文件夹的resolve_model.txt文件。\n\n当部署包以及模型准备完毕后，我们将模型（*.hbm）和部署包中的oellm_runtime SDK整合到一起，参考目录结构如下：\n\n\n\n\n板端运行准备#\n\n在板端创建工作目录，参考命令如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行#\n\n模型板端运行参考指令如下：\n\n\n\n其中，运行参数如下：\n\n参数                参数说明                            是否可选\n-h, --help        显示帮助信息。                         /\n--hbm_path        用于指定量化后的模型文件（*.hbm）路径。          必填\n--tokenizer_dir   用于指定分词器配置路径。                    必填\n--model_type      用于指定运行的模型类型，当前InternLM模型类型为4。   必填\n\n\n运行结果#\n\n运行完成后，即可进行简单对话测试，参考如下：\n\n","routePath":"/guide/quickstart/S100P/internlm2/simple_development_internlm2","lang":"zh","toc":[{"text":"模型及部署包准备","id":"模型及部署包准备","depth":2,"charIndex":82},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":306},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":463},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":767}],"domain":"","frontmatter":{},"version":"latest"},{"id":51,"title":"内存分配说明","content":"#\n\nS100和S100P的内存大小不同，各个模型的内存占用情况也不同。S100/S100P的底软提供了hb_switch_ion.sh脚本用来分配模型可使用的内存空\n间，该脚本主要有以下两种使用方式。\n\n\nbalanced模式#\n\n\n\n建议运行3B及以内的模型时使用balanced模式，若运行7B模型报错，请切换到bpu_first模式。\n\n\nbpu_first模式#\n\n\n\n在bpu_first模式下，模型可使用最多的内存空间，适合运行7B模型的场景。","routePath":"/guide/quickstart/S100P/ion_allocate","lang":"zh","toc":[{"text":"balanced模式","id":"balanced模式","depth":2,"charIndex":103},{"text":"bpu_first模式","id":"bpu_first模式","depth":2,"charIndex":173}],"domain":"","frontmatter":{},"version":"latest"},{"id":52,"title":"Qwen2.5模型开发","content":"#\n\n\n概述#\n\n本章节将为您介绍基于Qwen2.5模型的D-Robotics-LLM的全流程操作体系。\n这里将从简易开发与进阶开发两部分进行展开，覆盖不同技术场景下的使用流程，可根据您的实际使用场景选择。\n\n * 简易开发：本章节介绍了以Qwen2.5-1.5B-Instruct模型为例的简易开发流程，涵盖从模型和部署包准备到板端运行的关键步骤，帮助您快速验证Qwe\n   n2.5模型开发。\n\n * 进阶开发：本章节基于Qwen2.5模型的全周期开发体系，覆盖了从模型和部署包准备、模型量化、端侧部署、板端运行准备到板端运行的全流程开发链路，帮助您进一步\n   了解模型部署的全过程。\n\n\n支持模型#\n\nD-Robotics-LLM为您提供Qwen2.5模型量化和端侧部署功能。当前所支持的模型包括Qwen2.5-1.5B、Qwen2.5-7B、Qwen2.5-1\n.5B-Instruct和Qwen2.5-7B-Instruct。","routePath":"/guide/quickstart/S100P/qwen2_5","lang":"zh","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":3},{"text":"支持模型","id":"支持模型","depth":2,"charIndex":297}],"domain":"","frontmatter":{},"version":"latest"},{"id":53,"title":"进阶开发","content":"#\n\n在本章节，我们将为您介绍D-Robotics-LLM的进阶开发使用流程。\n\n此流程适用场景如下：\n\n 1. 已对模型进行微调（finetune），且需重新量化的场景。\n\n 2. 简单的单轮对话。\n\n 3. 连续多轮对话，模型可记忆前几轮的提问和回答。\n\n 4. 在端侧统计模型的PPL数值。\n\n\nQwen2.5模型版本选择#\n\n我们为您提供了Qwen2.5的Base和Instruct双版本模型，满足您不同的开发与应用需求，二者区别如下：\n\n * Base版本是基础的文本生成模型，适用于后续的模型训练任务，模型名称中不包含Instruct字样。\n\n * Instruct版本是在Base版本基础上经指令微调得到的模型，更适合用于对话场景，模型名称中包含Instruct字样。\n\n这里我们还是以Qwen2.5-1.5B-Instruct模型为例，为您进行使用说明。\n需要特别注意的是，连续多轮对话场景仅支持Instruct版本，其余场景均支持Base版本和Instruct版本。\n\n\n环境准备#\n\n请确保已按照 环境部署 章节正确完成开发机及开发板的环境准备。\n\n\n部署包准备#\n\n下载我们提供的D-Robotics_LLM_{version}.tar.gz部署包并解压。\n\n\n模型准备#\n\n注意\n\n当前仅支持Qwen2.5-1.5B、Qwen2.5-7B、Qwen2.5-1.5B-Instruct和Qwen2.5-7B-Instruct模型。\n在下载模型前，请确保您已了解模型的使用许可、依赖环境等必要信息，以保证模型后续能正常使用。\n\n您可以通过Hugging Face平台获取Qwen2.5系列的模型，以下为模型的下载链接：\n\n * Qwen2.5-1.5B模型\n * Qwen2.5-7B模型\n * Qwen2.5-1.5B-Instruct模型\n * Qwen2.5-7B-Instruct模型\n\n\n模型量化 #\n\nD-Robotics-LLM为您提供了使用命令行量化编译生成板端模型的功能，此时我们以Qwen2.5-1.5B-Instruct模型为例进行说明，参考命令如下：\n\n\n\n注解\n\n关于oellm_build工具的详细使用方法及注意事项，请参考 oellm_build工具 章节。\n\n若您通过resolve_model.txt提供的链接来获取我们编译好的hbm模型，则可跳过此模型量化步骤。\n\nresolve_model.txt文件中提供的Qwen2.5模型，均以chunk_size配置为256编译生成，且cache_len参数均使用1024。\n\n\n板端运行准备#\n\n在D-Robotics_LLM_{version}/oellm_runtime/example目录中，我们在各子目录提前准备好了编译后的可执行文件，可以直接在板\n端运行。您也可以执行不同的编译脚本，自行生成所需文件，参考命令如下：\n\n\n\n接下来在板端创建工作目录，参考命令如下：\n\n\n\n在上板之前，您需要确保已经准备如下内容：\n\n * 可运行的开发板，用于实际执行板端程序运行。\n\n * 一个可上板运行的模型（*.hbm），即 模型量化 的产出物。\n\n * 可执行文件（oellm_run，oellm_multichat和oellm_ppl文件）。\n\n * 板端程序依赖库，为了降低部署成本，您可以直接使用D-Robotics-LLM包内D-Robotics_LLM_{version}/oellm_runtim\n   e/set_performance_mode.sh、\n   D-Robotics_LLM_{version}/oellm_runtime/lib文件夹、D-Robotics_LLM_{version}/oellm_\n   runtime/config文件夹和D-Robotics_LLM_{version}/oellm_runtime/example文件夹中的内容。\n\n准备好之后，我们将模型文件（*.hbm）、可执行文件及依赖库整合到一起，参考目录结构如下：\n\n\n\n\n板端运行准备#\n\n在板端创建工作目录，参考命令如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行#\n\n\n简单会话#\n\n板端运行参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数                参数说明                                 是否可选\n-h, --help        显示帮助信息。                              /\n--hbm_path        用于指定量化后的模型文件（*.hbm）路径。               必填\n--tokenizer_dir   用于指定分词器配置路径。                         必填\n--template_path   用于指定Instruct模型的对话模板路径，加载Base模型时缺省。   可选\n--model_type      用于指定运行的模型类型，当前Qwen2.5模型类型为7。         必填\n\n\n多轮对话#\n\n板端运行参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数             参数说明              是否可选\n-h, --help     显示帮助信息。           /\n-c, --config   用于指定json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数              参数说明                           是否可选\nhbm_path        用于指定量化后的模型文件（*.hbm）路径。         必填\ntokenizer_dir   用于指定分词器配置路径。                   必填\ntemplate_path   用于指定对话模板路径。                    必填\nmodel_type      用于指定运行的模型类型，当前Qwen2.5模型类型为7。   必填\nbpu_core        用于指定使用的BPU核。默认值为-1，任意核。        可选\n\n\nPPL评估#\n\n板端统计模型的PPL数值参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数             参数说明              是否可选\n-h, --help     显示帮助信息。           /\n-c, --config   用于指定json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数              参数说明                                               是否可选\nhbm_path        用于指定量化后的模型文件（*.hbm）路径。                             必填\ntokenizer_dir   用于指定分词器配置路径。                                       必填\nmodel_type      用于指定运行的模型类型，当前Qwen2.5模型类型为7。                       必填\nppl_testcase    用于指定测试文件路径，当前仅支持bin格式。                             必填\nmax_length      用于指定每次送入模型的序列长度。                                   必填\nstride          用于指定测试步长。                                          必填\nbpu_core        用于指定使用的BPU核。默认值为-1，任意核。                            可选\nload_ckpt       是否读取上次测试中断后的进度信息继续测试，默认值为false。                    可选\ntext_data_num   用于指定截断文本到特定长度再测试，若text_data_num <= 0，则不截断，默认值为0。   可选\n\n\n运行结果#\n\n\n简单会话#\n\n简单对话测试参考如下：\n\n\n\n\n多轮对话#\n\n多轮对话测试参考如下：\n\n\n\n\nPPL评估#\n\nPPL评估完成后，会在同目录生成{ppl_testcase}.json文件，其中Perplexity所对应的数值，即为最终的PPL测试结果。参考如下：\n\n\n\n注解\n\n 1. PPL支持断点续测，程序运行时会在执行目录生成ppl_ckpt.json文件，当load_ckpt为true时会读取该文件，从中断处继续测试。\n\n 2. PPL程序执行结束后，会在bin测试文件所在目录生成json文件，包含本次测试的关键参数和PPL计算结果。\n\n 3. ppl_testcase参数设定的bin文件可从parquet转换得到，提供convert_parquet_to_bin.py示例参考代码如下：\n\n","routePath":"/guide/quickstart/S100P/qwen2_5/advanced_development_qwen","lang":"zh","toc":[{"text":"Qwen2.5模型版本选择","id":"qwen25模型版本选择","depth":2,"charIndex":150},{"text":"环境准备","id":"环境准备","depth":2,"charIndex":444},{"text":"部署包准备","id":"部署包准备","depth":2,"charIndex":485},{"text":"模型准备","id":"模型准备","depth":2,"charIndex":541},{"text":"模型量化","id":"模型量化","depth":2,"charIndex":-1},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":1089},{"text":"板端运行准备","id":"板端运行准备-1","depth":2,"charIndex":1693},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":1850},{"text":"简单会话","id":"简单会话","depth":3,"charIndex":1858},{"text":"多轮对话","id":"多轮对话","depth":3,"charIndex":2243},{"text":"PPL评估","id":"ppl评估","depth":3,"charIndex":2725},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":3616},{"text":"简单会话","id":"简单会话-1","depth":3,"charIndex":3624},{"text":"多轮对话","id":"多轮对话-1","depth":3,"charIndex":3647},{"text":"PPL评估","id":"ppl评估-1","depth":3,"charIndex":3670}],"domain":"","frontmatter":{},"version":"latest"},{"id":54,"title":"简易开发","content":"#\n\n本章节中，我们将为您介绍D-Robotics-LLM的基本使用流程，便于您实现快速上手。\n\n\nQwen2.5模型版本选择#\n\n我们为您提供了Qwen2.5的Base和Instruct双版本模型，满足您不同的开发与应用需求，二者区别如下：\n\n * Base版本是基础的文本生成模型，适用于后续的模型训练任务，模型名称中不包含Instruct字样。\n\n * Instruct版本是在Base版本基础上经指令微调得到的模型，更适合用于对话场景，模型名称中包含Instruct字样。\n\n这里我们以Qwen2.5-1.5B-Instruct模型为例，为您进行使用说明。\n\n\n模型及部署包准备#\n\n * 下载我们提供的D-Robotics_LLM_.tar.gz部署包并解压。\n\n * 下载我们提供的Qwen2.5_1.5B_Instruct_1024.hbm模型。\n\n提示\n\n关于hbm模型的下载链接，请参见oellm_runtime中model文件夹的resolve_model.txt文件。\n\n当部署包以及模型准备完毕后，我们将模型（*.hbm）和部署包中的oellm_runtime SDK整合到一起，参考目录结构如下：\n\n\n\n\n板端运行准备#\n\n在板端创建工作目录，参考命令如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行#\n\n模型板端运行参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数                参数说明                                 是否可选\n-h, --help        显示帮助信息。                              /\n--hbm_path        用于指定量化后的模型文件（*.hbm）路径。               必填\n--tokenizer_dir   用于指定分词器配置路径。                         必填\n--template_path   用于指定Instruct模型的对话模板路径，加载Base模型时缺省。   可选\n--model_type      用于指定运行的模型类型，当前Qwen2.5模型类型为7。         必填\n\n\n运行结果#\n\n运行完成后，即可进行简单对话测试，参考如下：\n\n","routePath":"/guide/quickstart/S100P/qwen2_5/simple_development_qwen","lang":"zh","toc":[{"text":"Qwen2.5模型版本选择","id":"qwen25模型版本选择","depth":2,"charIndex":49},{"text":"模型及部署包准备","id":"模型及部署包准备","depth":2,"charIndex":284},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":515},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":672},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":1059}],"domain":"","frontmatter":{},"version":"latest"},{"id":55,"title":"Qwen2.5-Omni模型开发","content":"#\n\n\n概述#\n\n本章节将为您介绍基于Qwen2.5-Omni模型的D-Robotics-LLM的全流程操作体系。\n这里将从简易开发与进阶开发两部分进行展开，覆盖不同技术场景下的使用流程，可根据您的实际使用场景选择。\n\n * 简易开发：本章节介绍了以Qwen2.5-Omni-3B模型为例的简易开发流程，涵盖从模型和部署包准备到板端运行的关键步骤，帮助您快速验证Qwen2.5-O\n   mni模型开发。\n\n * 进阶开发：本章节基于Qwen2.5-Omni模型的全周期开发体系，覆盖了从模型和部署包准备、模型量化、端侧部署、板端运行准备到板端运行的全流程开发链路，帮\n   助您进一步了解模型部署的全过程。\n\n\n支持模型#\n\nD-Robotics-LLM为您提供Qwen2.5-Omni模型量化和端侧部署功能。当前所支持的模型包括Qwen2.5-Omni-3B。","routePath":"/guide/quickstart/S100P/qwen2_5_omni","lang":"zh","toc":[{"text":"概述","id":"概述","depth":2,"charIndex":3},{"text":"支持模型","id":"支持模型","depth":2,"charIndex":306}],"domain":"","frontmatter":{},"version":"latest"},{"id":56,"title":"进阶开发","content":"#\n\n在本章节，我们将为您介绍D-Robotics-LLM的进阶开发使用流程。\n\n此流程适用场景如下：\n\n 1. 自行量化模型。\n\n 2. 离线运行，通过读取本地的音频、视频、图像等数据，模型生成文本回答。\n\n 3. 在线运行，通过流式传输音频、视频数据，模型生成文本回答。相比离线运行，在线运行可以一边传输一边处理，从而显著降低模型生成首字回答的延时。\n\n针对以上几种场景，这里我们还是以Qwen2.5_Omni_3B模型为例，为您进行使用说明。\n\n\n环境准备#\n\n请确保已按照 环境部署 章节正确完成开发机及开发板的环境准备。\n\n\n部署包准备#\n\n下载我们提供的D-Robotics_LLM_{version}.tar.gz部署包并解压。\n\n\n模型准备#\n\n注意\n\n当前仅支持Qwen2.5-Omni-3B模型。在下载模型前，请确保您已了解模型的使用许可、依赖环境等必要信息，以保证模型后续能正常使用。\n\n您可以通过Hugging Face平台获取Omni系列的模型，以下为模型的下载链接：\n\n * Qwen/Qwen2.5-Omni-3B模型\n\n\n模型量化 #\n\nD-Robotics-LLM为您提供了使用命令行量化编译生成板端模型的功能，此时我们以Qwen2.5-Omni-3B模型为例进行说明，参考命令如下：\n\n\n\n注解\n\n关于oellm_build工具的详细使用方法及注意事项，请参考 oellm_build工具 章节。\n\n若您通过resolve_model.txt提供的链接来获取我们编译好的hbm模型，则可跳过此模型量化步骤。\n\nresolve_model.txt文件中提供的Omni模型，均以chunk_size配置为256编译生成，且cache_len参数使用2048配置，且当前仅支持\n该配置。\n\n\n多模态支持 #\n\nQwen2.5_Omni_3B模型支持的模态包括文本、音频、图片和视频。无论在任意输入组合下，模型的输出均为纯文本。\n\n多模态支持分为离线运行和在线运行两种模式，二者所支持的输入组合存在一些差异，具体如下：\n\n\n离线运行#\n\n序号   文本    音频    图片    视频\n1    Y     N/A   N/A   N/A\n2    N/A   Y     N/A   N/A\n3    N/A   N/A   Y     N/A\n4    N/A   N/A   N/A   Y\n5    Y     N/A   Y     N/A\n6    N/A   Y     Y     N/A\n7    Y     N/A   N/A   Y\n8    N/A   Y     N/A   Y\n\n * 文本内容在json中填写，不单独准备文本文件。\n\n * 音频格式支持mp3，wav和flac，最多支持30秒长度。\n\n * 图片格式支持jpg，png，bmp和jpeg，长宽会固定缩放到448x448。\n\n * 视频格式支持mp4和mkv，最多支持5秒，每秒会解析2帧，长宽会固定缩放到448x448。此外，若视频中有音频且无单独的音频输入，则视频中的音频会一并解析\n   ，若有单独的音频输入，则视频中的音频无效。\n\n所有模态的输入均需通过json文件配置，详细说明可参考 板端运行 章节。\n\n\n在线运行#\n\n序号   文本    音频    视频\n1    Y     N/A   Y\n2    N/A   Y     Y\n3    N/A   Y     N/A\n\n * 对于文本内容，您可使用接口xlm_omni_feed_text_online向模型传输。\n\n * 视频格式仅支持nv12，您可使用接口xlm_omni_feed_video_online向模型传输单帧的nv12数据，长宽会被固定缩放到448x448，单\n   次对话支持传输2-10帧的nv12数据。\n\n * 音频数据类型仅支持float32，分布范围[-1,\n   1]，您可使用接口xlm_omni_feed_audio_online向模型单次传输完整音频，或多次传输音频片段，单次对话支持累计最多30s音频。\n\n\n板端运行准备#\n\n在D-Robotics_LLM_{version}/oellm_runtime/example目录中，我们在各子目录提前准备好了编译后的可执行文件，可以直接在板\n端运行。您也可以执行不同的编译脚本，自行生成所需文件，参考命令如下：\n\n\n\n接下来在板端创建工作目录，参考命令如下：\n\n\n\n在运行之前，您需要确保已经准备好如下内容：\n\n * 可运行的开发板，用于实际执行板端程序。\n\n * 可上板运行的模型（*.hbm）。\n\n * 模型的输入嵌入权重（embed_tokens.bin文件）。\n\n * 可执行文件（oellm_omni_offline和oellm_omni_online文件）及对应的json配置文件。\n\n * 板端程序依赖库，为了降低部署成本，您可以直接使用D-Robotics-LLM包内D-Robotics_LLM_{version}/oellm_runtim\n   e/set_performance_mode.sh，\n   D-Robotics_LLM_{version}/oellm_runtime/lib文件夹，D-Robotics_LLM_{version}/oellm_\n   runtime/config文件夹和D-Robotics_LLM_{version}/oellm_runtime/example文件夹中的内容。\n\n准备好之后，我们将上述文件整合到一起，目录的参考结构如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行 #\n\n\n离线运行#\n\n离线运行参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数             参数说明                  是否可选\n-h, --help     显示帮助信息。               /\n-c, --config   用于指定运行时的json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数                参数说明                                     是否可选\nvisual_hbm_path   用于指定量化后的视频图像特征提取模型文件（*.hbm）路径。           必填\naudio_hbm_path    用于指定量化后的音频特征提取模型文件（*.hbm）路径。             必填\ntext_hbm_path     用于指定量化后的文本模型文件（*.hbm）路径。                 必填\nembed_tokens      用于指定模型的输入嵌入权重（embed_tokens.bin）路径。       必填\ntokenizer_dir     用于指定分词器和部分初始化数据的配置路径。                    必填\nmodel_type        用于指定运行的模型类型，当前Omni模型类型为5。                必填\nonline_mode       用于指定模型以在线或者离线方式运行。取值范围：'true'、'false'。   必填\n\n在程序运行时，您还需在命令行单独输入一份准备了多模态输入信息的json文件的路径，再按下回车启动交互。\n\n离线运行支持的模态包括文本、音频、图片和视频，您需在json文件中提前写好输入信息并保存在本地，模板如下：\n\n注意\n\n注：当前json文件模板仅作模板示例展示，若需了解输入模态支持的组合输入形式，请参考 多模态支持 小节。\n\n\n\n其中，在json配置文件模板中，同一conversation节点下包含system的text字段，以及user部分的text，audio，image和video\n字段。若无需使用某种模态输入，需将该模态对应的完整大括号内容删除。 例如，仅输入视频时，json文件可按照如下方式配置：\n\n\n\n\n在线运行#\n\nD-Robotics-LLM部署包提供了API，支持以流式传输的方式在线运行Qwen2.5_Omni_3B模型，为此我们提供了在线运行示例供参考。\n该示例的板端运行参考指令如下：\n\n\n\n程序的输入参数如下：\n\n参数             参数说明                  是否可选\n-h, --help     显示帮助信息。               /\n-c, --config   用于指定运行时的json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数                参数说明                                     是否可选\nvisual_hbm_path   用于指定量化后的视频图像特征提取模型文件（*.hbm）路径。           必填\naudio_hbm_path    用于指定量化后的音频特征提取模型文件（*.hbm）路径。             必填\ntext_hbm_path     用于指定量化后的文本模型文件（*.hbm）路径。                 必填\nembed_tokens      用于指定模型的输入嵌入权重（embed_tokens.bin）路径。       必填\ntokenizer_dir     用于指定分词器和部分初始化数据的配置路径。                    必填\nmodel_type        用于指定运行的模型类型，当前Omni模型类型为5。                必填\nonline_mode       用于指定模型以在线或者离线方式运行。取值范围：'true'、'false'。   必填\nvideo_path        用于指定在线运行时解析的视频文件路径。                      必填\nuser_text         用于指定user的文本输入内容。                         可选\n\n提示\n\n在线运行的音频数据解析自视频，若您使用自有视频数据进行测试，建议使用带音频的视频。\n\n\n运行结果#\n\n\n离线运行#\n\n一轮完整的离线运行交互方式如下，其中[User] <<<\n的./omni_offline_prompt.json为您所提供的含有输入信息的json文件所在路径，[Assistant] >>>\n对应模型的文本输出，输出文本前会将json文件中的输入信息打印至终端。\n\n\n\n\n在线运行#\n\n输入1、2和3可依次运行在线模式支持的三种输入组合，输入0为退出程序，在线运行的完整交互功能演示如下：\n\n","routePath":"/guide/quickstart/S100P/qwen2_5_omni/advanced_development_omni","lang":"zh","toc":[{"text":"环境准备","id":"环境准备","depth":2,"charIndex":226},{"text":"部署包准备","id":"部署包准备","depth":2,"charIndex":267},{"text":"模型准备","id":"模型准备","depth":2,"charIndex":323},{"text":"模型量化","id":"模型量化","depth":2,"charIndex":-1},{"text":"多模态支持","id":"多模态支持","depth":2,"charIndex":-1},{"text":"离线运行","id":"离线运行","depth":3,"charIndex":875},{"text":"在线运行","id":"在线运行","depth":3,"charIndex":1371},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":1721},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":-1},{"text":"离线运行","id":"离线运行-1","depth":3,"charIndex":2477},{"text":"在线运行","id":"在线运行-1","depth":3,"charIndex":3480},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":4419},{"text":"离线运行","id":"离线运行-2","depth":3,"charIndex":4427},{"text":"在线运行","id":"在线运行-2","depth":3,"charIndex":4570}],"domain":"","frontmatter":{},"version":"latest"},{"id":57,"title":"简易开发","content":"#\n\n本章节中，我们将为您介绍D-Robotics-LLM的基本使用流程，便于您实现快速上手。这里我们以Qwen2.5-Omni-3B模型为例，为您进行使用说明。\n\n\n模型及部署包准备#\n\n * 下载我们提供的D-Robotics_LLM_.tar.gz部署包并解压。\n\n * 下载我们提供的Qwen2.5_Omni_3B_Audio.hbm，Qwen2.5_Omni_3B_Visual.hbm，Qwen2.5_Omni_3B_T\n   ext.hbm模型，以及模型的输入嵌入权重embed_tokens.bin文件。\n\n提示\n\n关于hbm模型的下载链接，请参见oellm_runtime中model文件夹的resolve_model.txt文件。\n\n准备完毕后，我们将模型（*.hbm），embed_tokens.bin文件，以及部署包中的oellm_runtime SDK整合到一起，参考目录结构如下：\n\n\n\n\n板端运行准备#\n\n在板端创建工作目录，参考命令如下：\n\n\n\n将开发机中整合的文件夹拷贝至此板端目录下，参考命令如下：\n\n\n\n最后，在/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime路径下，对LD_LIBRARY_PATH进行配\n置，参考命令如下：\n\n\n\n\n板端运行#\n\nQwen2.5_Omni_3B模型有online和offline两种运行模式。这里以offline模式为例，模型板端运行参考指令如下：\n\n\n\n在offline模式中，可执行文件的参数如下：\n\n参数             参数说明                  是否可选\n-h, --help     显示帮助信息。               /\n-c, --config   用于指定运行时的json配置文件路径。   必填\n\njson配置文件示例如下：\n\n\n\njson配置文件参数说明如下：\n\n参数                参数说明                                      是否可选\nvisual_hbm_path   用于指定量化后的视频图像特征提取模型文件（*.hbm）路径。            必填\naudio_hbm_path    用于指定量化后的音频特征提取模型文件（*.hbm）路径。              必填\ntext_hbm_path     用于指定量化后的文本模型文件（*.hbm）路径。                  必填\nembed_tokens      用于指定模型的输入嵌入权重（embed_tokens.bin）路径。        必填\ntokenizer_dir     用于指定分词器和部分初始化数据的配置路径。                     必填\nmodel_type        用于指定运行的模型类型，当前Omni模型类型为5。                 必填\nonline_mode       用于指定模型以在线或者离线方式运行。取值范围： 'true'、'false'。   必填\n\n\n运行结果#\n\n运行后，即可进行测试，参考如下：\n\n\n\nQwen2.5_Omni_3B模型运行时需传入json文件的路径，您需在该json文件中配置音频、视频、图像、文本等输入内容。程序运行过程中，会将json文件中\n的输入信息同步打印至终端。 在该示例中，omni_offline_prompt.json文件内容如下：\n\n\n\n关于多模态输入的完整支持范围、在线运行模式的具体说明，以及json文件的详细填写规范等内容，请参考 进阶开发 章节。","routePath":"/guide/quickstart/S100P/qwen2_5_omni/simple_development_omni","lang":"zh","toc":[{"text":"模型及部署包准备","id":"模型及部署包准备","depth":2,"charIndex":83},{"text":"板端运行准备","id":"板端运行准备","depth":2,"charIndex":408},{"text":"板端运行","id":"板端运行","depth":2,"charIndex":565},{"text":"运行结果","id":"运行结果","depth":2,"charIndex":1332}],"domain":"","frontmatter":{},"version":"latest"},{"id":58,"title":"oellm_build工具","content":"#\n\noellm_build工具是地瓜提供的将原始浮点模型映射为量化模型的工具。基于原始浮点模型、json配置文件（可选）和校准数据（可选）完成模型的量化及编译，最终\n生成可用于部署的*.hbm模型。\n\n\n使用方法#\n\n\n\n\n命令行参数#\n\n\njson配置文件说明 #\n\n 1. 文本calibrate数据json配置文件，参考示例如下：\n    \n    \n\n 2. Qwen2.5-Omni模型所需calibrate数据，参考示例如下：\n    \n    \n\n配置文件参数说明：\n\n（1） 当\"role\"为\"system\"时，content列表中的第一个元素必须是文本元素，且包含text字段，否则会在格式化模板时访问不到text而报错。\n\n注解\n\n支持在同一conversation中存在多个system消息，但是首条必须是文本元素，且包含text字段。除首条以外，其他system消息支持类型包含text，\naudio，image，video。\n\n（2） 当\"role\"为\"user\"时，content列表中支持类型包含text，audio，image，video。具体规则如下：\n\n注解\n\ncontent列表支持两种消息组织形式：\n\n 1. 同一类型的消息：可包含单个或多个（如多个text消息、多个image消息，多个video消息，多个audio消息）。\n\n 2. 不同类型的消息：可将多种类型组合（如text+image+audio消息搭配）。\n\n * 当content列表中类型为\"text\"时：\n   \n   * 格式限制：无特殊格式要求，纯文本、带标点的句子、短句指令、长段落等均支持。\n   \n   * 来源支持：无固定来源限制。\n   \n   * 参考示例：\n   \n   \n\n * 当content列表中类型为\"video\"时：\n   \n   * 格式限制：MP4、MKV。\n   \n   * 来源支持：本地视频文件、本地文件URL（file://）、网络URL（http(s)://）。\n   \n   * 参考示例：\n   \n   \n\n * 当content列表中类型为\"image\"时：\n   \n   * 格式限制：PNG、JPG、JPEG、BMP。\n   \n   * 来源支持：本地图像文件、本地文件URL（file://）、网络URL（http(s)://）、Data URI。\n   \n   * 参考示例：\n   \n   \n\n * 当content列表中类型为\"audio\"时：\n   \n   * 格式限制：WAV、MP3、FLAC。\n   \n   * 来源支持：本地音频文件、本地文件URL（file://）、网络URL（http(s)://）、Data URI。\n   \n   * 参考示例：\n   \n   \n\n\n使用示例#\n\n * DeepSeek-R1-Distill-Qwen模型使用oellm_build工具进行模型量化，参考命令如下：\n   \n   \n\n * DeepSeek-R1-Distill-Qwen模型使用oellm_build工具进行模型量化，并对量化生成的HBM模型进行一致性校验，参考命令如下：\n   \n   \n\n * InternLM2模型使用oellm_build工具进行模型量化，参考命令如下：\n   \n   \n\n * Qwen2.5模型使用oellm_build工具进行模型量化，参考命令如下：\n   \n   \n\n * Qwen2.5-Omni模型使用oellm_build工具进行模型量化，参考命令如下：\n   \n   ","routePath":"/guide/tool_introduction/oellm_build","lang":"zh","toc":[{"text":"使用方法","id":"使用方法","depth":2,"charIndex":102},{"text":"命令行参数","id":"命令行参数","depth":2,"charIndex":112},{"text":"json配置文件说明","id":"json配置文件说明","depth":2,"charIndex":-1},{"text":"使用示例","id":"使用示例","depth":2,"charIndex":1177}],"domain":"","frontmatter":{},"version":"latest"}]