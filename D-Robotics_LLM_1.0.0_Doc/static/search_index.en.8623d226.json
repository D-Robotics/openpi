[{"id":0,"title":"Main Interface Functions","content":"#\n\n\nCreate Default Parameter#\n\n\n\nCreate default model general parameters\n\n * Return Value\n   \n   * Construct the xlm_common_params_t structure data with default parameter.\n\n\nInitialize the Instance#\n\n\n\nInitialize the instance.\n\n * Parameters\n   \n   * [in]: param, general parameters of the model generated during\n     initialization.\n   \n   * [in]: callback, the callback function pointer for registering a task, i.e.,\n     the execution entity of the task.\n   \n   * [out]: llm_handle, the inference handle, which is used for the management\n     of subsequent tasks.\n\n * Return Value\n   \n   * 0 (Initialization successful), -1 (Initialization failed).\n\n\nSynchronous Inference#\n\n\n\nSynchronous inference, starting the inference includes a complete prefill and\ndecode process.\n\n * Parameters\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n   \n   * [in]: input, the model inference input, including data such as prompt,\n     image, and task priority.\n   \n   * [in]: userdata, user-defined data, which is returned through the callback\n     function along with the inference result.\n\n * Return Value\n   \n   * 0 (Inference task executed successfully), -1 (Failed to obtain the\n     inference handle, task returned).\n\n\nPPL Calculation#\n\n\n\nThis is used only for PPL calculation and will not be executed for regular\ntasks.\n\n * Parameters\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n   * [in]: input, input for model inference, typically text or wikitest data.\n   * [in]: userdata,user-defined data, which is returned through the callback\n     function along with the inference result.\n\n * Return Value\n   \n   * 0 (PPL calculation task executed successfully), -1 (failed to obtain the\n     inference handle, task returned).\n\n\nAsynchronous Inference#\n\n\n\nAsynchronous inference, starting the inference includes a complete prefill and\ndecode process.\n\n * Parameters\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n   \n   * [in]: input, the model inference input, including data such as prompt,\n     image, and task priority.\n   \n   * [in]: userdata, user-defined data, which is returned through the callback\n     function along with the inference result.\n\n * Return Value\n   \n   * 0 (Inference task executed successfully), -1 (Failed to obtain the\n     inference handle, task returned).\n\n\nDestroy the Instance#\n\n\n\nRelease the inference instance resources.\n\n * Parameters\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n\n * Return Value\n   \n   * 0 (Task destroyed successfully), -1 (Failed to obtain the inference handle,\n     interface returned).\n\n\nOmni Audio Input#\n\n\n\nProvide audio input when Omni is running online.\n\n * Parameters:\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n   * [in]: audio_input, audio input, including the memory start address and\n     length information.\n\n * Return Value:\n   \n   * 0 (Correctly transmit audio data), -1 (Failed to obtain audio data, task\n     returned).\n\n\nOmni Video Input#\n\n\n\nProvide video input when Omni is running online.\n\n * Parameters:\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n   * [in]: video_input, video input, including the start addresses of the Y and\n     UV components and their width and height.\n\n * Return Value:\n   \n   * 0 (Correctly transmit video data), -1 (Failed to obtain video data, task\n     returned).\n\n\nOmni Text Input#\n\n\n\nProvide text input when Omni is running online.\n\n * Parameters:\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n   * [in]: text_input, text input, including system text and user text.\n\n * Return Value:\n   \n   * 0 (Correctly transmit text data), -1 (Failed to obtain text data, task\n     returned).\n\n\nOmni Synchronous Inference#\n\n\n\nStart Omni's full processing pipeline in a synchronous manner.\n\n * Parameters:\n   \n   * [in]: handle, the inference handle obtained through the xlm_init interface.\n   * [in]: input, the model inference input.\n   * [in]: userdata, user-defined data, which is returned through the callback\n     function along with the inference result.\n\n * Return Value:\n   \n   * 0 (Inference task executed successfully), -1 (Failed to obtain the\n     inference handle, task returned).","routePath":"/en/guide/api/api_details/api_functions","lang":"en","toc":[{"text":"Create Default Parameter","id":"create-default-parameter","depth":2,"charIndex":3},{"text":"Initialize the Instance","id":"initialize-the-instance","depth":2,"charIndex":173},{"text":"Synchronous Inference","id":"synchronous-inference","depth":2,"charIndex":653},{"text":"PPL Calculation","id":"ppl-calculation","depth":2,"charIndex":1248},{"text":"Asynchronous Inference","id":"asynchronous-inference","depth":2,"charIndex":1793},{"text":"Destroy the Instance","id":"destroy-the-instance","depth":2,"charIndex":2390},{"text":"Omni Audio Input","id":"omni-audio-input","depth":2,"charIndex":2687},{"text":"Omni Video Input","id":"omni-video-input","depth":2,"charIndex":3077},{"text":"Omni Text Input","id":"omni-text-input","depth":2,"charIndex":3493},{"text":"Omni Synchronous Inference","id":"omni-synchronous-inference","depth":2,"charIndex":3850}],"domain":"","frontmatter":{},"version":"latest"},{"id":1,"title":"Basic Type Definition","content":"#\n\n\nHandle Type#\n\n\n\n * Used to represent the handle of an inference instance.","routePath":"/en/guide/api/api_details/basic_type_definition","lang":"en","toc":[{"text":"Handle Type","id":"handle-type","depth":2,"charIndex":3}],"domain":"","frontmatter":{},"version":"latest"},{"id":2,"title":"Callback Function Type","content":"#\n\n\n\n * Inference result callback function. It returns the inference results and\n   status, and is registered when the initialization interface is called.","routePath":"/en/guide/api/api_details/callback","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":3,"title":"Structure and Enumeration Types","content":"#\n\n\nSampling Parameters#\n\n\n\n * Parameter settings for the model output sampling algorithm.\n\n\nModel Types#\n\n\n\n * Specify the model type. Currently, deepseek, internlm, omni and qwen2.5 are\n   supported.\n\n\nGeneral Parameters#\n\n\n\n * Configuration of model general parameters. The default parameters can be\n   obtained via the xlm_create_default_param interface.\n\n\nInput Types#\n\n\n\n * The input type of the request.\n\n\nInference Backend Type#\n\n\n\n * Inference backend type setting, which supports the BPU core binding function.\n\n\nImage Preprocessing type#\n\n\n\n * Image preprocessing type setting. Currently, only dynamic resolution is\n   supported.\n\n\nToken Input Structure#\n\n\n\n * The structure of the token input type.\n\n\nImage Input Structure#\n\n\n\n * The image structure in multimodal input.\n\n\nMultimodal Input Structure#\n\n\n\n * The structure in multimodal input.\n   \n   * Supports single prompt and single image.\n   \n   * Supports single prompt with multiple images. (not support yet)\n\n\nOmni Video Input Structure#\n\n\n\n * Structure of online video input parameters for the Omni model.\n\n\nOmni Audio Input Structure#\n\n\n\n * Structure of online audio input parameters for the Omni model.\n\n\nOmni Text Input Structure#\n\n\n\n * Structure of online text input parameters for the Omni model.\n\n\nPriority Type#\n\n\n\n * Model input request priority settings.\n   \n   * The preemption relationships are as follows:\n     \n     * XLM_PRIORITY_TYPE_URGENT --preemption--> XLM_PRIORITY_TYPE_HIGH\n       --preemption--> XLM_PRIORITY_TYPE_NORMAL.\n   \n   * When the priority is both NORMAL, preemption will not occur. Instead, the\n     execution order will be determined based on the value of priority.\n     \n     * A higher priority value indicates a higher priority. The value range is\n       [0, 253].\n\n\nPPL Parameter Structure#\n\n\n\n * The parameter structure for the model's PPL, and currently only supports the\n   PPL of the InternVL model. */}\n\n\nSingle Inference Request Structure#\n\n\n\n * Single inference request structure. (In the future, it will support multiple\n   requests at the same time.)\n\n\nInference Input Structure#\n\n\n\n * Model inference input structure, which is passed as a parameter to the\n   inference interface.\n\n\nPerformance Data Structure#\n\n\n\n * Model performance data structure. At the end of inference, it will return the\n   performance data of this inference.\n\n\nInference Result Structure#\n\n\n\n * The structure for returning inference result.\n\n\nInterface Status#\n\n\n\n * The current inference status of the model is returned along with the\n   inference results.","routePath":"/en/guide/api/api_details/structure_and_enumeration_type","lang":"en","toc":[{"text":"Sampling Parameters","id":"sampling-parameters","depth":2,"charIndex":3},{"text":"Model Types","id":"model-types","depth":2,"charIndex":92},{"text":"General Parameters","id":"general-parameters","depth":2,"charIndex":203},{"text":"Input Types","id":"input-types","depth":2,"charIndex":360},{"text":"Inference Backend Type","id":"inference-backend-type","depth":2,"charIndex":412},{"text":"Image Preprocessing type","id":"image-preprocessing-type","depth":2,"charIndex":522},{"text":"Token Input Structure","id":"token-input-structure","depth":2,"charIndex":642},{"text":"Image Input Structure","id":"image-input-structure","depth":2,"charIndex":712},{"text":"Multimodal Input Structure","id":"multimodal-input-structure","depth":2,"charIndex":784},{"text":"Omni Video Input Structure","id":"omni-video-input-structure","depth":2,"charIndex":977},{"text":"Omni Audio Input Structure","id":"omni-audio-input-structure","depth":2,"charIndex":1076},{"text":"Omni Text Input Structure","id":"omni-text-input-structure","depth":2,"charIndex":1175},{"text":"Priority Type","id":"priority-type","depth":2,"charIndex":1272},{"text":"PPL Parameter Structure","id":"ppl-parameter-structure","depth":2,"charIndex":1771},{"text":"Single Inference Request Structure","id":"single-inference-request-structure","depth":2,"charIndex":1915},{"text":"Inference Input Structure","id":"inference-input-structure","depth":2,"charIndex":2067},{"text":"Performance Data Structure","id":"performance-data-structure","depth":2,"charIndex":2197},{"text":"Inference Result Structure","id":"inference-result-structure","depth":2,"charIndex":2350},{"text":"Interface Status","id":"interface-status","depth":2,"charIndex":2432}],"domain":"","frontmatter":{},"version":"latest"},{"id":4,"title":"Overview","content":"#\n\nBy reading this section, you can learn how to use API interfaces to load, infer,\nand process the results of LLM model. The basic flowchart of interface calls is\nshown below:\n\n\n\nRegarding the API interfaces for LLM model inference, they include content such\nas basic type definition, structure and enumeration types, callback function\ntype, and main interface functions. For details, please refer to the Interface\nDetails section.","routePath":"/en/guide/api/api_overview","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":5,"title":"Benchmark of DeepSeek-R1-Distill-Qwen Model Performance","content":"#\n\n\nTest Conditions#\n\n * Test Board：S100P。\n\n * Performance Data Acquisition: Test a single prompt and record the metrics of\n   TTFT (Time to First Token) and TPS (Average Tokens Per Second).\n\n * Python version：Python3.10。\n\n * Runtime Environment：Linux。\n\n\nMeasured data#","routePath":"/en/guide/benchmark/deepseek_r1_distill_qwen","lang":"en","toc":[{"text":"Test Conditions","id":"test-conditions","depth":2,"charIndex":3},{"text":"Measured data","id":"measured-data","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":6,"title":"Benchmark of InternLM2 Model Performance","content":"#\n\n\nTest Conditions#\n\n * Test Board：S100P。\n\n * Performance Data Acquisition: Test a single prompt and record the metrics of\n   TTFT (Time to First Token) and TPS (Average Tokens Per Second).\n\n * Python version：Python3.10。\n\n * Runtime Environment：Linux。\n\n\nMeasured data#","routePath":"/en/guide/benchmark/internlm2","lang":"en","toc":[{"text":"Test Conditions","id":"test-conditions","depth":2,"charIndex":3},{"text":"Measured data","id":"measured-data","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":7,"title":"Benchmark of Qwen2.5 Model Performance","content":"#\n\n\nTest Conditions#\n\n * Test Board：S100P。\n\n * Performance Data Acquisition: Test a single prompt and record the metrics of\n   TTFT (Time to First Token) and TPS (Average Tokens Per Second).\n\n * Python version：Python3.10。\n\n * Runtime Environment：Linux。\n\n\nMeasured data#","routePath":"/en/guide/benchmark/qwen2.5","lang":"en","toc":[{"text":"Test Conditions","id":"test-conditions","depth":2,"charIndex":3},{"text":"Measured data","id":"measured-data","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":8,"title":"Benchmark of Qwen2.5-Omni Model Performance","content":"#\n\n\nTest Conditions#\n\n * Test Board：S100P。\n\n * Performance Data Acquisition: Test a single prompt and record the metrics of\n   TTFT (Time to First Token) and TPS (Average Tokens Per Second).\n\n * Python version：Python3.10。\n\n * Runtime Environment：Linux。\n\n\nMeasured data#","routePath":"/en/guide/benchmark/qwen2.5_omni","lang":"en","toc":[{"text":"Test Conditions","id":"test-conditions","depth":2,"charIndex":3},{"text":"Measured data","id":"measured-data","depth":2,"charIndex":-1}],"domain":"","frontmatter":{},"version":"latest"},{"id":9,"title":"D-Robotics-LLM Document Introduction","content":"#\n\nThis document is intended for all developers using D-Robotics-LLM. It provides\ncomprehensive guidance throughout your development process. To help you fully\nunderstand the overall usage workflow, D-Robotics recommends reading this\ndocument in the following order.\n\nBelow is a brief overview of each section. You may also navigate directly to\nspecific chapters based on your needs.\n\n1. LLM Document Introduction\n\nThis section provides you with brief introductions to and navigation links for\nrelevant sections in the overall document, as well as the recommended reading\norder of the document.\n\n 2. Product Introduction\n\n 3. Environmental Deployment\n\nThis section outlines the pre-required environment deployment steps and details,\nyou need to complete for the development environment.\n\n 4. Usage Instructions\n\n 5. Tools Introduction\n\n 6. API Introduction\n\nThis section introduces the API interfaces to help you better understand and use\nthe API.\n\n 7. Benchmark of Model Performance\n\n 8. D-Robotics-LLM Algorithm Toolchain License Agreement\n\nThis section provides the D-Robotics-LLM algorithm toolchain license agreement.\nYou must read it carefully before using the D-Robotics-LLM toolchain.","routePath":"/en/guide/doc_introduction","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":10,"title":"Environmental Deployment","content":"#\n\nThis chapter introduces how to complete environment setup on the x86 development\nmachine and on the S100/S100P edge devices.\n\n\nx86 Quantization Environment Setup#\n\n\nRecommended Hardware Configuration#\n\nCPU: Intel(R) Core(TM) i9-14900K, 32 cores\n\nGPU: NVIDIA RTX 3090\n\nRAM: 128 GB\n\nWarning\n\nIf the hardware configuration is lower, quantization may be slower or could\nfail.\n\n\nDeployment Package Preparation#\n\nDownload the D-Robotics_LLM_{version}.tar.gz installation package provided by us\nand extract it correctly.\n\n\nInstall the Conda Environment#\n\nNote\n\nIf you have already installed the Conda environment, please skip this step.\n\nTo facilitate the management of the Python environment, we recommend using\nMiniforge3 as the Python package management tool.\n\n 1. Download the Miniforge3 installation package:\n    \n    \n\n 2. Install the Miniforge3:\n    \n    \n\n\nSet Up the Conda Environment#\n\n 1. Enter the Conda base environment, refer to the following command:\n    \n    \n\n 2. Create a python3.10 environment and switch to the environment, refer to the\n    following command:\n    \n    \n    \n    Enter the oellm conda environment:\n    \n    \n    \n    Install the required Python environment:\n    \n    \n\n\nInstall the other files in the deployment package#\n\nAfter setting up the Conda environment, you need to continue installing the\nmodel quantization, conversion, and compilation toolkits included in the LLM\ndeployment package. Refer to the following commands:\n\n\n\nAt this point, we have completed the setup of the model quantization and\ncompilation environment on the x86 development machine. If you need to\ncross-compile C++ edge-side executable files, you may proceed with the following\nsteps.\n\n\nCross-Compilation Toolchain Configuration#\n\nWithin the LLM deployment package, we provide an installation package for the\ncross-compilation toolchain, which is used to cross-compile ARM programs on the\nx86 platform. Simply create a directory on the x86 device and extract the\ninstallation package. Refer to the following commands:\n\nWarning\n\nPlease note that installing to the /opt directory requires root privileges. If\nyou do not have root access, you may choose to extract it to another directory\ninstead.\n\n\n\nAfter extraction, configure the environment variables to check the build\ntoolchain, using the following commands as a reference:\n\n\n\nAt this point, you can cross-compile ARM executable programs on the x86\nplatform. Each example in oellm_runtime provides a build script for your use.\n\n\nDevelopment Board Runtime Environment Setup#\n\nPlease refer to the Memory Allocation Instructions to complete the memory\nallocation procedure on the development board.","routePath":"/en/guide/env_install","lang":"en","toc":[{"text":"x86 Quantization Environment Setup","id":"x86-quantization-environment-setup","depth":2,"charIndex":129},{"text":"Recommended Hardware Configuration","id":"recommended-hardware-configuration","depth":3,"charIndex":167},{"text":"Deployment Package Preparation","id":"deployment-package-preparation","depth":3,"charIndex":376},{"text":"Install the Conda Environment","id":"install-the-conda-environment","depth":3,"charIndex":518},{"text":"Set Up the Conda Environment","id":"set-up-the-conda-environment","depth":3,"charIndex":860},{"text":"Install the other files in the deployment package","id":"install-the-other-files-in-the-deployment-package","depth":3,"charIndex":1201},{"text":"Cross-Compilation Toolchain Configuration","id":"cross-compilation-toolchain-configuration","depth":3,"charIndex":1696},{"text":"Development Board Runtime Environment Setup","id":"development-board-runtime-environment-setup","depth":2,"charIndex":2491}],"domain":"","frontmatter":{},"version":"latest"},{"id":11,"title":"D-Robotics-LLM Algorithm Toolchain License Agreement","content":"#\n\nAttention\n\n**IMPORTANT NOTICE - Please read the following terms carefully before using the\nToolchain. **\n\nIf you are under 18 years old or have not obtained the necessary authorization,\nor if you do not accept any and all of the following terms, please refrain from\nusing the Toolchain. By using this Toolchain, you are affirming that you are\nfully aware of, understand and accept the following terms.\n\nThis D-Robotics-LLM Algorithm Toolchain License Agreement(hereinafter referred\nto as the\"Agreement\") is entered into and between you (whether an individual, an\nenterprise or other entity, all simply referred to as \"You\") and D-Robotics,\nwhich governs your use of the toolchain software package and related\ndocumentation materials (referred to as the \"Toolchain\") provided by D-Robotics.\n\n 1.  Definition\n     \n     1.1. \"Intellectual Property Rights\" refer to the intellectual property\n     rights and other intangible rights stipulated by the laws of any\n     jurisdiction, including but not limited to patents(invention patents,\n     utility model patents, design patents, etc.), copyrights(including property\n     rights and moral rights), trademarks, trade names, packaging, decoration,\n     data, databases, trade secrets, confidential information, know-how,\n     proprietary technology, regardless of whether the rights are registered or\n     unregistered. Without limiting the foregoing, the Intellectual Property\n     Rights hereunder also include the rights to apply, be granted, renewed or\n     extended, and to claim priority, as well as all present and future similar\n     or equivalent rights.\n     \n     1.2. \"Affiliates\" means any other legal entity that, directly or\n     indirectly, controls, is controlled by, or is under common control with the\n     party; \"Control\" including the related terms \"controlled\" and \"is under\n     common control\", means having the power, whether present or future,\n     directly or indirectly, through voting rights, contractual arrangements or\n     otherwise, to determine the direction of the management and policies of a\n     controlled legal entity; and, without prejudice to the foregoing, any legal\n     entity shall be deemed to have such power of control over any controlled\n     legal entity to the extent that such legal entity owns or controls at least\n     fifty percent(50%) of the voting and/or proxy voting power of any\n     outstanding voting stock or other form of ownership interest in such\n     controlled legal entity.\n\n 2.  License\n     \n     2.1. Subject to your compliance with the terms of this Agreement,\n     D-Robotics grants You a limited, non-exclusive, non-transferable,\n     non-sublicensable and revocable license(at any time) to install and use\n     copies of the Toolchain.\n     \n     2.2. D-Robotics reserves all rights that are not expressly granted to You.\n     Except for the license expressly granted to You in Article 2.1 hereof,\n     nothing in this Agreement shall be construed as licensing or transferring\n     any ownership, Intellectual Property Rights or other interests to You by\n     express, implied or any other means.\n\n 3.  Limitations\n     \n     The following limitations and restrictions apply to your use of the\n     Toolchain. You are responsible for the consequences of non-conformance with\n     these limitations:\n     \n     3.1. You shall not modify the Toolchain in any form;\n     \n     3.2. You shall not reverse engineer, disassemble, decompile the Toolchain,\n     or attempt in any other manner to obtain source code, internal structure or\n     design of the Toolchain;\n     \n     3.3. Except as expressly authorized herein, You shall not sell, lease,\n     sublicense, distribute, transfer or in any other form to provide or\n     disclose the Toolchain to any third party;\n     \n     3.4. You shall not use the Toolchain in any manner that would cause the\n     Toolchain or any part thereof to become subject to an open-source software\n     license agreement;\n     \n     3.5. You shall not remove any copyright or other proprietary rights\n     statements or labels from the Toolchain;\n     \n     3.6. You shall not use the Toolchain for the purpose of developing\n     competing products or technologies or assisting a third party in such\n     activities;\n     \n     3.7. You shall not apply for or register any form of Intellectual Property\n     Rights regarding the Toolchain in any jurisdiction. Otherwise, You shall\n     immediately transfer, as required by D-Robotics, such Intellectual Property\n     Rights to D-Robotics or a third party designated by D-Robotics free of\n     charge;\n     \n     3.8. You shall not utilize the Toolchain to support any infringement or\n     unfair competition lawsuits or any similar claims against D-Robotics,\n     D-Robotics' Affiliates, D-Robotics' suppliers, and their respective\n     customers, nor file any applications that may undermine or affect the\n     validity of the registered or applied Intellectual Property Rights of\n     D-Robotics, D-Robotics' Affiliates, D-Robotics' suppliers, and their\n     respective customers.\n\n 4.  Disclaimer\n     \n     4.1. THE TOOLCHAIN IS PROVIDED BY HORIZON ON AN \"AS IS\" BASIS. HORIZON\n     MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND(EXPRESS OR IMPLIED)\n     RELATING TO OR ARSING UNDER THE AGREEMENT, INCLUDING, WITHOUT LIMITATION,\n     THE WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY, FITNESS FOR A\n     PARTICULAR PURPOSE, COMPATIBILITY, CORRECTNESS, ACCURACY, INTEGRITY,\n     RELIABILITY, PERFORMANCE OR NON-DEFECT, NOR WILL ASSUME ANY QUALITY\n     LIABILITY FOR ANY KNOWN OR UNKNOWN ERRORS OR DEFECTS, INCLUDING BUT NOT\n     LIMITED TO PROVIDING NOTICES, CORRECTIONS, MODIFICATIONS OR RELEASING\n     UPGRADE PATCHES. WITHOUT LIMITING THE FOREGOING, HORIZON DOES NOT WARRANT\n     THAT THE TOOLCHAIN WILL MEET YOUR REQUIREMENTS, THAT ANY DEFECTS OR ERRORS\n     WILL BE CORRECTED, THAT ANY CERTAIN CONTENT WILL BE AVAILABLE, OR THAT THE\n     TOOLCHAIN IS FREE OF VIRUSES OR OTHER HARMFUL COMPONENTS.\n     \n     4.2. UNDER NO CIRCUMSTANCES SHALL HORIZON BE LIABLE FOR ANY DIRECT,\n     INDIRECT, CONSEQUENTIAL, INCIDENTAL OR PUNITIVE DAMAGES SUFFERED BY YOU\n     ARISING FROM THE USE OR INABILITY TO USE PART OR ALL OF THE TOOLCHAIN,\n     INCLUDING BUT NOT LIMITED TO LOSSES FOR THE LOST REVENUE, PROFITS, DATA OR\n     BUSINESS INTERRUPTION, COSTS OF PROCURING SUBSTITIVE GOODS, AND THIRD-PARTY\n     CLAIMS RELATING TO INTELLECTUAL PROPERTY INFRINGEMENT OR UNFAIR\n     COMPETITION, ADN EVEN IF HORIZON HAS BEEN ADVISED OF THE POSSIBILITY OF\n     SUCH DAMAGES.\n     \n     4.3. You agree to indemnify D-Robotics and its Affiliates, employees,\n     customers, partners and other relevant personnel against any claims,\n     liabilities, damages, losses or costs arising from your use of the\n     Toolchain, including but not limited to third-party claims and legal fees.\n\n 5.  Feedback\n     \n     You may, but are not obligated to, provide suggestions, requests,\n     modifications, improvements, or other feedback related to this\n     Toolchain(collectively referred to as \"Feedback\") to D-Robotics or a\n     D-Robotics Affiliate. If You provide Feedback, the relevant Intellectual\n     Property Rights thereof shall belong to D-Robotics.\n\n 6.  Termination\n     \n     6.1. If You fail to comply with any terms of this Agreement, or if You\n     initiate or participate in any lawsuits, arbitrations or any legal action\n     against D-Robotics, its Affiliates, and/or its customers based on the\n     Toolchain, D-Robotics is entitled the right to immediately and unilaterally\n     terminate this Agreement. You must cease using the Toolchain and destroy\n     all copies of the Toolchain in your possession once the Agreement is\n     terminated.\n     \n     6.2. You may terminate this Agreement at any time by ceasing to use the\n     Toolchain and destroying all copies of the Toolchain in your possession.\n     \n     6.3. Except for Article 2(\"License\") of this Agreement, all other\n     provisions hereof shall survive the termination of this Agreement.\n\n 7.  Assignment\n     \n     You may not assign or transfer any of your rights or obligations under this\n     Agreement without the prior written consent of D-Robotics, and any attempt\n     to do so shall be void and null.\n\n 8.  Export Control Compliance\n     \n     8.1. You agree and undertake: (i) to strictly comply with all applicable\n     laws and regulations relating to import and export control including China,\n     the European Union and the United States of America(collectively referred\n     to as \"Export Control Laws and Regulations\", including but not limited to\n     the Export Administration Regulations(EAR) of the U.S. government); and\n     (ii) not to violate the restrictive provisions of Export Control Laws and\n     Regulations on items, destinations, end users, and end uses.\n     \n     8.2. If required by authorities or D-Robotics to conduct an export control\n     compliance review, You shall, upon receipt of such request, promptly\n     provide authorities or D-Robotics with information regarding the end users,\n     destinations, and end uses related to the Toolchain, and cooperate with the\n     compliance review.\n     \n     8.3. Notwithstanding any other provisions of this Agreement, D-Robotics has\n     the right to terminate this Agreement immediately and unilaterally in the\n     event that You violate Export Control Laws and Regulations, or you and your\n     Affiliates are listed on any sanction lists of individuals or entities\n     prohibited by the relevant government from engaging in export\n     activities(collectively \"Sanction Lists\"). You shall immediately cease to\n     use the Toolchain and destroy all copies of the Toolchain in your\n     possession once the Agreement is terminated.\n\n 9.  Laws and Jurisdiction\n     \n     9.1. The conclusion, validity, interpretation, performance, and dispute\n     resolution of this Agreement are subject to the laws of the People's\n     Republic of China, excluding the application of the conflict of laws\n     principles or the United Nations Convention on Contracts for the\n     International Sale of Goods.\n     \n     9.2. Both parties agree that in case of any dispute, they shall resolve it\n     through friendly negotiation. If the negotiation fails, either party may\n     submit the dispute to the Beijing Arbitration Commission(\"BAC\") for\n     arbitration in accordance with the current rules of the Commission. The\n     place of arbitration is in Beijing. The arbitration award is final and\n     binding on You and D-Robotics.\n\n 10. Others\n     \n     10.1. Regarding the subject matter of the Agreement, the parties agree that\n     the Agreement constitutes the entire and exclusive agreement between the\n     parties regarding the use and offering of the Toolchain and supersedes all\n     prior and contemporaneous communications and commitments. Any additional or\n     different terms or conditions will not be binding and are null and void.\n     This Agreement can only be modified in writing by affixing the contract\n     seal or official seal of both parties.\n     \n     10.2. If any provision of this Agreement is unenforceable or invalid for\n     any reason, it shall not affect the validity of the remaining provisions of\n     this Agreement, which will remain in full force and effect.\n     \n     10.3. This Agreement is available in both Chinese and English versions. In\n     case of any inconsistency between the two versions, the Chinese version\n     shall prevail.","routePath":"/en/guide/license_agreement","lang":"en","toc":[],"domain":"","frontmatter":{},"version":"latest"},{"id":12,"title":"Development Process","content":"#\n\nThe overall development steps of Open D-Robotics-LLM mainly include environment\ndeployment, large model quantization, and end-side deployment.\n\n\nEnvironment Deployment#\n\nIn this stage, you need to complete the correct installation and deployment of\nthe development environment as required, to proceed with the subsequent steps.\nFor details, please refer to the Environment Deployment section.\n\n\nLarge Model Quantization#\n\nIn this stage, you need to download the required large language model from\nHugging Face. We will convert this model into a format that can be deployed on\nthe D-Robotics platform for subsequent inference. The steps include:\n\n 1. Obtain the original model. Supported model types include:\n    \n    * Open-source Hugging Face format models:\n      DeepSeek-R1-Distill-Qwen-1.5B、DeepSeek-R1-Distill-Qwen-7B、InternLM2-1.8B、Q\n      wen2.5-1.5B、Qwen2.5-7B、Qwen2.5-1.5B-Instruct、Qwen2.5-7B-Instruct、Qwen2.5-O\n      mni-3B.\n    \n    * Custom-trained large language models, which must have the same structure\n      as the Hugging Face format models listed above.\n\n 2. Model quantization (you can also skip this step by directly using the\n    converted models we provide):\n    \n    Use the oellm_build command-line tool for conversion and compilation,\n    building model that can be deployed on the D-Robotics platform. During the\n    build process, you can set quantization parameters according to your actual\n    needs. The generated *.hbm file can be used for subsequent board-side\n    deployment.\n\n\nBoard-side Deployment#\n\nIn this stage, you can deploy and run inference with the quantized models\nobtained in the previous stage. The steps include:\n\n 1. Cross-compilation toolchain configuration: correctly extract the\n    cross-compilation toolchain package and configure the path.\n\n 2. Use the provided build scripts to generate executable files for board-side\n    deployment.\n\n 3. Board-side inference: after preparing all models, executables, and\n    dependencies for board-side deployment, run the executable on the board\n    device. Once completed, you can perform inference and dialogue tests.\n\n 4. Resource release: after completing the workflow, destroy the inference task\n    interface and release occupied resources.\n\nEnvironment deployment, model quantization, and board-side deployment together\nform the complete D-Robotics-LLM development process, ensuring that large\nlanguage models can be successfully converted, optimized, and efficiently\ndeployed on the D-Robotics platform.","routePath":"/en/guide/preface/develop_process","lang":"en","toc":[{"text":"Environment Deployment","id":"environment-deployment","depth":2,"charIndex":147},{"text":"Large Model Quantization","id":"large-model-quantization","depth":2,"charIndex":397},{"text":"Board-side Deployment","id":"board-side-deployment","depth":2,"charIndex":1514}],"domain":"","frontmatter":{},"version":"latest"},{"id":13,"title":"Product Introduction","content":"#\n\n\nWhat is D-Robotics-LLM#\n\nD-Robotics-LLM Toolchain is a specialized extension of D-Robotics'\nself-developed S100 toolchain product for large model applications. It enables\nthe development, conversion, and deployment of large language models on the S100\nseries platforms and derivative platforms. The reference solutions developed\nbased on this toolchain provide case support for large model requirements in\nrobotics.\n\nAfter obtaining the D-Robotics-LLM release package, you can get started by\nfollowing these steps:\n\n 1. First, refer to the Release Content section to understand the directory\n    structure of the release package.\n\n 2. Next, refer to the Development Process section for a brief overview of the\n    overall usage flow of the deployment package.\n\n 3. Then, refer to the Environment Deployment section to correctly complete the\n    required environment installation and deployment.\n\n 4. Finally, according to the model you are using, refer to the corresponding\n    model section in S100/S100P平台 to complete the full process from model\n    conversion to deployment.\n\nFor more comprehensive D-Robotics-LLM tutorials, please refer to the subsequent\nsections.\n\n\nRelease Content #\n\n\nS100/S100P#\n\nThe structure of the D-Robotics-LLM deployment package is as follows:\n\n\n\nNote\n\nThis is only a demonstration of the deployment package structure. Please refer\nto the actual release package you use for specific version numbers.\n\nAmong them:\n\n * arm-gnu-toolchain-11.3.rel1-x86_64-aarch64-none-linux-gnu.tar.xz is the\n   cross-compilation tool package required by the deployment package.\n\n * The oellm_build folder contains environment dependencies, compiler, and leap\n   unified operator library dependencies.\n   \n   * The requirements.txt is the environment dependencies required for setting\n     up the development environment.\n   \n   * The hbdk4_compiler-{version}-cp310-cp310-manylinux_2_17_x86_64.whl is the\n     installation package for Horizon's hbdk4 compiler.\n   \n   * The hbdk4_runtime_aarch64_unknown_linux_gnu_nash-{version}-py3-none-any.whl\n     is the installation package for the hbdk4 runtime.\n   \n   * The leap_llm-{version}-py310-none-any.whl is the installation package for\n     leap-llm, providing model quantization and conversion functions.\n\n * The oellm_runtime folder contains dependencies and examples for the\n   board-side deployment SDK.\n   \n   * The model provides download links for supported models and scripts for\n     one-click download of all models.\n   \n   * The config contains reference configuration files for supported models and\n     configuration files required at runtime.\n   \n   * The example contains demo source code, build scripts, binaries, and runtime\n     configuration files.\n   \n   * The include contains key header files required for compiling edge-side\n     deployment examples.\n   \n   * The lib contains libraries required by the edge-side deployment program.\n   \n   * The set_performance_mode.sh script is used to modify hardware register\n     values to set the device to performance mode. Currently, only the S100P is\n     supported.","routePath":"/en/guide/preface/preface","lang":"en","toc":[{"text":"What is D-Robotics-LLM","id":"what-is-d-robotics-llm","depth":2,"charIndex":3},{"text":"Release Content","id":"release-content","depth":2,"charIndex":-1},{"text":"S100/S100P","id":"s100s100p","depth":3,"charIndex":1194}],"domain":"","frontmatter":{},"version":"latest"},{"id":14,"title":"S100/S100P Platform","content":"#\n\n\nOverview#\n\nThis section introduces the usage workflow of D-Robotics-LLM based on the\nS100/S100P platform. It provides memory allocation guidelines and detailed usage\nprocedures for the DeepSeek-R1-Distill-Qwen, InternLM2, Qwen2.5, and\nQwen2.5-Omni models, helping you gain a comprehensive understanding of the\nentire process.\n\n * Memory Allocation Instructions: This section explains on-device memory\n   allocation. On the S100/S100P platform, you can employ different memory\n   allocation strategies to meet the runtime requirements of various models.\n\n * DeepSeek-R1-Distill-Qwen Model Development: This section presents the\n   complete operational workflow of D-Robotics-LLM based on the\n   DeepSeek-R1-Distill-Qwen model, divided into Simple Development and Advanced\n   Development to illustrate usage procedures under different technical\n   scenarios.\n\n * InternLM2 Model Development: This section presents the complete operational\n   workflow of D-Robotics-LLM based on the InternLM2 model, divided into Simple\n   Development and Advanced Development to illustrate usage procedures under\n   different technical scenarios.\n\n * Qwen2.5 Model Development: This section presents the complete operational\n   workflow of D-Robotics-LLM based on the Qwen2.5 model, divided into Simple\n   Development and Advanced Development to illustrate usage procedures under\n   different technical scenarios.\n\n * Qwen2.5-Omni Model Development: This section presents the complete\n   operational workflow of D-Robotics-LLM based on the Qwen2.5-Omni model,\n   divided into Simple Development and Advanced Development to illustrate usage\n   procedures under different technical scenarios.\n\n\nSupported Models#\n\nOn the S100/S100P platform, D-Robotics-LLM currently supports the following\nmodels:\n\n * DeepSeek-R1-Distill-Qwen: DeepSeek-R1-Distill-Qwen-1.5B and\n   DeepSeek-R1-Distill-Qwen-7B.\n\n * InternLM2: InternLM2-1.8B。.\n\n * Qwen2.5: Qwen2.5-1.5B、Qwen2.5-7B、Qwen2.5-1.5B-Instruct and\n   Qwen2.5-7B-Instruct.\n\n * Qwen2.5-Omni: Qwen2.5-Omni-3B.","routePath":"/en/guide/quickstart/S100P","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"Supported Models","id":"supported-models","depth":2,"charIndex":1677}],"domain":"","frontmatter":{},"version":"latest"},{"id":15,"title":"DeepSeek-R1-Distill-Qwen Model Development","content":"#\n\n\nOverview#\n\nThis section introduces the end-to-end operational framework of D-Robotics-LLM\nbased on the DeepSeek-R1-Distill-Qwen model.\n\nIt is divided into two parts—simple development and advanced\ndevelopment—covering usage workflows for different technical scenarios. You can\nchoose the appropriate approach according to your actual use case.\n\n * Simple Development: This section describes a streamlined development workflow\n   using the DeepSeek-R1-Distill-Qwen-1.5B model as an example, covering key\n   steps from model and deployment package preparation to on-device execution,\n   enabling you to quickly validate DeepSeek-R1-Distill-Qwen model development.\n\n * Advanced Development: This section presents a full-cycle development pipeline\n   based on the DeepSeek-R1-Distill-Qwen model, encompassing the entire\n   workflow—from model and deployment package preparation, model quantization,\n   edge-side deployment, on-device runtime setup, to final on-device\n   execution—helping you gain a deeper understanding of the complete model\n   deployment process.\n\n\nSupported Models#\n\nD-Robotics-LLM provides quantization and edge-side deployment capabilities for\nthe DeepSeek-R1-Distill-Qwen model. Currently supported models include\nDeepSeek-R1-Distill-Qwen-1.5B and DeepSeek-R1-Distill-Qwen-7B.","routePath":"/en/guide/quickstart/S100P/deepseek_r1_distill_qwen","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"Supported Models","id":"supported-models","depth":2,"charIndex":1067}],"domain":"","frontmatter":{},"version":"latest"},{"id":16,"title":"Advanced Development","content":"#\n\nIn this chapter, we will introduce the advanced development workflow of\nD-Robotics-LLM.\n\nThis workflow applies to the following scenarios:\n\n 1. The model has already been fine-tuned and requires re-quantization.\n\n 2. Simple single-turn conversations.\n\n 3. Continuous multi-turn conversations, where the model can remember questions\n    and answers from previous rounds.\n\n 4. Calculating the model's Perplexity (PPL) metric on the edge device.\n\nFor the above scenarios, we will use the DeepSeek-R1-Distill-Qwen-1.5B model as\nan example to illustrate usage.\n\n\nEnvironment Setup#\n\nPlease ensure you have correctly completed environment setup for both the\ndevelopment host and development board according to the Environment Deployment\nsection.\n\n\nDeployment Package Preparation#\n\nDownload the provided D-Robotics_LLM_{version}.tar.gz deployment package and\nextract it.\n\n\nModel Preparation#\n\nNote\n\nCurrently, only the DeepSeek-R1-Distill-Qwen-1.5B and\nDeepSeek-R1-Distill-Qwen-7B models are supported.\n\nBefore downloading a model, please ensure you understand its license terms,\ndependency requirements, and other necessary information to guarantee proper\nsubsequent usage.\n\nYou can obtain DeepSeek series models via the Hugging Face platform. Below are\nthe download links:\n\n * DeepSeek-R1-Distill-Qwen-1.5B model\n * DeepSeek-R1-Distill-Qwen-7B model\n\n\nModel Quantization #\n\nD-Robotics-LLM provides command-line tools to quantize and compile models for\nedge devices. Here, we demonstrate using the DeepSeek-R1-Distill-Qwen-1.5B model\nwith the following reference command:\n\n\n\nNote\n\nFor detailed usage instructions and precautions regarding the oellm_build tool,\nplease refer to the oellm_build Tool section.\n\nIf you obtain our pre-compiled .hbm models via the links provided in\nresolve_model.txt, you may skip this quantization step.\n\nAll DeepSeek models listed in resolve_model.txt were compiled with\nchunk_size=256 and two different cache_len configurations (1024 and 4096). This\ndifference is reflected in their filenames.\n\n\nEdge Device Runtime Preparation#\n\nIn the directory D-Robotics_LLM_{version}/oellm_runtime/example, we have\npre-prepared compiled executables in respective subdirectories that can be\ndirectly run on the edge device. Alternatively, you can execute different build\nscripts to generate required files yourself. Reference commands are as follows:\n\n\n\nNext, create a working directory on the edge device with the following command:\n\n\n\nBefore deploying to the board, ensure you have prepared the following:\n\n * A functional development board for running programs.\n * A deployable model file (*.hbm), which is the output from Model Quantization.\n * Executable files (oellm_run, oellm_multichat, and oellm_ppl).\n * Runtime dependency libraries. To reduce deployment overhead, you can directly\n   use the contents from the following directories within the D-Robotics-LLM\n   package:\n   * D-Robotics_LLM_{version}/oellm_runtime/set_performance_mode.sh\n   * D-Robotics_LLM_{version}/oellm_runtime/lib\n   * D-Robotics_LLM_{version}/oellm_runtime/config\n   * D-Robotics_LLM_{version}/oellm_runtime/example\n\nAfter preparation, organize your model files (*.hbm), executables, and\ndependencies into a unified directory structure as shown below:\n\n\n\nCopy the integrated folder from your development host to the edge device\ndirectory using the following command:\n\n\n\nFinally, configure LD_LIBRARY_PATH under\n/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime with the following\ncommands:\n\n\n\n\nRunning on Edge Device#\n\n\nSimple Conversation#\n\nReference command for running on the edge device:\n\n\n\nProgram input parameters are as follows:\n\nPARAMETER         DESCRIPTION                                                OPTIONAL/REQUIRED\n-h, --help        Display help information.                                  /\n--hbm_path        Path to the quantized model file (*.hbm).                  Required\n--tokenizer_dir   Path to tokenizer configuration.                           Required\n--template_path   Path to conversation template.                             Required\n--model_type      Specifies the model type; currently, DeepSeek models use   Required\n                  type 1.\n\n\nMulti-turn Conversation#\n\nReference command for running on the edge device:\n\n\n\nProgram input parameters are as follows:\n\nPARAMETER      DESCRIPTION                            OPTIONAL/REQUIRED\n-h, --help     Display help information.              /\n-c, --config   Path to the JSON configuration file.   Required\n\nExample JSON configuration file:\n\n\n\nJSON configuration parameters are described below:\n\nPARAMETER       DESCRIPTION                                                   OPTIONAL/REQUIRED\nhbm_path        Path to the quantized model file (*.hbm).                     Required\ntokenizer_dir   Path to tokenizer configuration.                              Required\ntemplate_path   Path to conversation template.                                Required\nbpu_core        Specifies the BPU core to use. Default value is -1, meaning   Optional\n                any core.\n\n\nPPL Evaluation#\n\nTo evaluate the model's PPL (Perplexity) on the device, refer to the following\ncommands:\n\n\n\nThe program accepts the following input arguments:\n\nARGUMENT       DESCRIPTION                                        OPTIONAL/REQUIRED\n-h, --help     Display help information.                          /\n-c, --config   Specify the path to the JSON configuration file.   Required\n\nExample of the JSON configuration file:\n\n\n\nDescriptions of the JSON configuration parameters are as follows:\n\nPARAMETER       DESCRIPTION                                                    OPTIONAL/REQUIRED\nhbm_path        Specifies the path to the quantized model file (*.hbm).        Required\ntokenizer_dir   Specifies the path to the tokenizer configuration.             Required\nmodel_type      Specifies the model type to run. The current DeepSeek model    Required\n                type is 1.\nppl_testcase    Specifies the path to the test file. Currently, only bin       Required\n                format is supported.\nmax_length      Specifies the sequence length fed into the model each time.    Required\nstride          Specifies the stride length used during evaluation.            Required\nbpu_core        Specifies the BPU core to use. Default value is -1, meaning    Optional\n                any core.\nload_ckpt       Whether to resume testing from the last interruption point.    Optional\n                Default value is false.\ntext_data_num   Specifies truncating the input text to a specific length       Optional\n                before testing. If text_data_num <= 0, no truncation occurs.\n                Default value is 0.\n\n\nExecution Results#\n\n\nSimple Conversation#\n\nExample of a simple conversation test:\n\n\n\n\nMulti-turn Dialogue#\n\nExample of a multi-turn dialogue test:\n\n\n\n\nPPL Evaluation#\n\nAfter completing the PPL evaluation, a {ppl_testcase}.json file will be\ngenerated in the same directory. The value corresponding to Perplexity in this\nfile represents the final PPL result. Example output:\n\n\n\nNote\n\n 1. PPL evaluation supports resuming from checkpoints. During execution, the\n    program generates a ppl_ckpt.json file in the working directory. When\n    load_ckpt is set to true, the program reads this file and resumes testing\n    from the interruption point.\n\n 2. After the PPL program finishes execution, a JSON file containing key test\n    parameters and the PPL result will be generated in the same directory as the\n    bin test file.\n\n 3. The bin file specified by the ppl_testcase parameter can be converted from a\n    parquet file. An example script convert_parquet_to_bin.py is provided below:\n\n","routePath":"/en/guide/quickstart/S100P/deepseek_r1_distill_qwen/advanced_development_deepseek","lang":"en","toc":[{"text":"Environment Setup","id":"environment-setup","depth":2,"charIndex":560},{"text":"Deployment Package Preparation","id":"deployment-package-preparation","depth":2,"charIndex":744},{"text":"Model Preparation","id":"model-preparation","depth":2,"charIndex":868},{"text":"Model Quantization","id":"model-quantization","depth":2,"charIndex":-1},{"text":"Edge Device Runtime Preparation","id":"edge-device-runtime-preparation","depth":2,"charIndex":2023},{"text":"Running on Edge Device","id":"running-on-edge-device","depth":2,"charIndex":3496},{"text":"Simple Conversation","id":"simple-conversation","depth":3,"charIndex":3522},{"text":"Multi-turn Conversation","id":"multi-turn-conversation","depth":3,"charIndex":4185},{"text":"PPL Evaluation","id":"ppl-evaluation","depth":3,"charIndex":5058},{"text":"Execution Results","id":"execution-results","depth":2,"charIndex":6691},{"text":"Simple Conversation","id":"simple-conversation-1","depth":3,"charIndex":6712},{"text":"Multi-turn Dialogue","id":"multi-turn-dialogue","depth":3,"charIndex":6777},{"text":"PPL Evaluation","id":"ppl-evaluation-1","depth":3,"charIndex":6842}],"domain":"","frontmatter":{},"version":"latest"},{"id":17,"title":"Simple Development","content":"#\n\nIn this chapter, we will introduce the basic usage workflow of D-Robotics-LLM to\nhelp you get started quickly. Here, we take the DeepSeek-R1-Distill-Qwen-1.5B\nmodel as an example to demonstrate its usage.\n\n\nModel and Deployment Package Preparation#\n\n * Download the provided D-Robotics_LLM_.tar.gz deployment package and extract\n   it.\n\n * Download the provided DeepSeek_R1_Distill_Qwen_1.5B_1024.hbm model.\n\nNote\n\nFor the download link of the hbm model, please refer to the resolve_model.txt\nfile in the model folder of oellm_runtime.\n\nAfter preparing both the deployment package and the model, integrate the model\n(*.hbm) with the oellm_runtime SDK from the deployment package. The reference\ndirectory structure is as follows:\n\n\n\n\nOn-Device Runtime Preparation#\n\nCreate a working directory on the device using the following commands as a\nreference:\n\n\n\nCopy the integrated folder from your development machine to this device\ndirectory. Reference command:\n\n\n\nFinally, configure LD_LIBRARY_PATH under the path\n/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime. Reference commands:\n\n\n\n\nOn-Device Execution#\n\nReference command for running the model on the device:\n\n\n\nThe runtime parameters are as follows:\n\nPARAMETER         DESCRIPTION                                                OPTIONAL/REQUIRED\n-h, --help        Display help information.                                  /\n--hbm_path        Specifies the path to the quantized model file (*.hbm).    Required\n--tokenizer_dir   Specifies the tokenizer configuration path.                Required\n--template_path   Specifies the conversation template path.                  Required\n--model_type      Specifies the model type to run; currently, the DeepSeek   Required\n                  model type is 1.\n\n\nExecution Results#\n\nA simple dialogue test example is shown below:\n\n","routePath":"/en/guide/quickstart/S100P/deepseek_r1_distill_qwen/simple_development_deepseek","lang":"en","toc":[{"text":"Model and Deployment Package Preparation","id":"model-and-deployment-package-preparation","depth":2,"charIndex":209},{"text":"On-Device Runtime Preparation","id":"on-device-runtime-preparation","depth":2,"charIndex":735},{"text":"On-Device Execution","id":"on-device-execution","depth":2,"charIndex":1090},{"text":"Execution Results","id":"execution-results","depth":2,"charIndex":1765}],"domain":"","frontmatter":{},"version":"latest"},{"id":18,"title":"InternLM2 Model Development","content":"#\n\n\nOverview#\n\nThis section introduces the complete operational workflow of D-Robotics-LLM\nbased on the InternLM2 model.\n\nIt is divided into two parts—simple development and advanced\ndevelopment—covering usage procedures for different technical scenarios. You can\nchoose the appropriate approach according to your actual use case.\n\n * Simple Development: This section describes a streamlined development process\n   using the InternLM2-1.8B model as an example, covering key steps from model\n   and deployment package preparation to on-device execution, enabling you to\n   quickly validate InternLM2 model development.\n\n * Advanced Development: This section presents a full-cycle development\n   framework based on the InternLM2 model, encompassing the entire workflow—from\n   model and deployment package preparation, model quantization, edge-side\n   deployment, on-device runtime setup, to final on-device execution—helping you\n   gain a deeper understanding of the complete model deployment process.\n\n\nSupported Models#\n\nD-Robotics-LLM provides quantization and edge-side deployment capabilities for\nthe InternLM2 model. Currently supported models include InternLM2-1.8B.","routePath":"/en/guide/quickstart/S100P/internlm2","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"Supported Models","id":"supported-models","depth":2,"charIndex":1002}],"domain":"","frontmatter":{},"version":"latest"},{"id":19,"title":"Advanced Development","content":"#\n\nIn this chapter, we will introduce the advanced development workflow of\nD-Robotics-LLM.\n\nThis workflow applies to the following scenarios:\n\n 1. Cases where the model has already been fine-tuned and requires\n    re-quantization.\n\n 2. Simple single-turn conversations.\n\n 3. Calculating the model's Perplexity (PPL) metric on edge devices.\n\nFor the above scenarios, we will use the InternLM2-1.8B model as an example to\nillustrate usage.\n\n\nEnvironment Setup#\n\nPlease ensure you have correctly completed environment setup for both the\ndevelopment host and the development board as described in the Environment\nDeployment section.\n\n\nDeployment Package Preparation#\n\nDownload the provided D-Robotics_LLM_{version}.tar.gz deployment package and\nextract it.\n\n\nModel Preparation#\n\nNote\n\nCurrently, only the InternLM2-1.8B model is supported.\n\nBefore downloading the model, please ensure you understand the model's license\nterms, dependency requirements, and other necessary information to guarantee\nproper subsequent usage.\n\nYou can obtain the InternLM2 series models from the Hugging Face platform. Below\nare the download links:\n\n * InternLM2-1.8B Model\n\n\nModel Quantization #\n\nD-Robotics-LLM provides a command-line tool to quantize and compile models for\nedge-device deployment. Taking the InternLM2-1.8B model as an example, the\nreference command is as follows:\n\n\n\nNote\n\nFor detailed usage instructions and important considerations regarding the\noellm_build tool, please refer to the oellm_build Tool section.\n\nIf you obtain our pre-compiled .hbm model via the link provided in\nresolve_model.txt, you may skip this model quantization step.\n\nThe InternLM2 model provided in the resolve_model.txt file was compiled with\nchunk_size set to 256 and cache_len set to 1024.\n\n\nEdge Device Runtime Preparation#\n\nIn the directory D-Robotics_LLM_{version}/oellm_runtime/example, we have\npre-prepared compiled executables in each subdirectory that can be directly run\non the edge device. Alternatively, you can execute different build scripts to\ngenerate the required files yourself. Reference commands are as follows:\n\n\n\nNext, create a working directory on the edge device with the following command:\n\n\n\nBefore deploying to the board, ensure you have prepared the following:\n\n * A functional development board for running programs on the edge device.\n\n * A deployable model file (*.hbm), which is the output from Model Quantization.\n\n * Executable files (oellm_run and oellm_ppl).\n\n * Runtime dependency libraries. To reduce deployment overhead, you can directly\n   use the contents within the D-Robotics-LLM package, including:\n   \n   * D-Robotics_LLM_{version}/oellm_runtime/set_performance_mode.sh\n   * D-Robotics_LLM_{version}/oellm_runtime/lib folder\n   * D-Robotics_LLM_{version}/oellm_runtime/config folder\n   * D-Robotics_LLM_{version}/oellm_runtime/example folder\n\nAfter preparation, integrate the model file (*.hbm), executable files, and\ndependency libraries into a unified directory structure as shown below:\n\n\n\nCopy the integrated folder from your development host to the edge device using\nthe following command:\n\n\n\nFinally, configure the LD_LIBRARY_PATH under the path\n/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime with the following\ncommands:\n\n\n\n\nEdge Device Execution#\n\n\nSimple Conversation#\n\nReference command for running on the edge device:\n\n\n\nRuntime parameters are as follows:\n\nPARAMETER         DESCRIPTION                                                  OPTIONAL/REQUIRED\n-h, --help        Display help information.                                    /\n--hbm_path        Specifies the path to the quantized model file (*.hbm).      Required\n--tokenizer_dir   Specifies the tokenizer configuration directory.             Required\n--model_type      Specifies the model type; currently, InternLM uses type 4.   Required\n\n\nPPL Evaluation#\n\nReference command for calculating the model's PPL on the edge device:\n\n\n\nProgram input parameters are as follows:\n\nPARAMETER      DESCRIPTION                                          OPTIONAL/REQUIRED\n-h, --help     Display help information.                            /\n-c, --config   Specifies the path to the JSON configuration file.   Required\n\nExample JSON configuration file:\n\n\n\nJSON configuration parameters are explained below:\n\nPARAMETER       DESCRIPTION                                                   OPTIONAL/REQUIRED\nhbm_path        Specifies the path to the quantized model file (*.hbm).       Required\ntokenizer_dir   Specifies the tokenizer configuration directory.              Required\nmodel_type      Specifies the model type; currently, InternLM2 uses type 4.   Required\nppl_testcase    Specifies the test file path; currently supports only bin     Required\n                format.\nmax_length      Specifies the sequence length fed into the model per          Required\n                iteration.\nstride          Specifies the testing stride.                                 Required\nbpu_core        Specifies the BPU core to use. Default is -1 (any core).      Optional\nload_ckpt       Whether to resume testing from the last interruption point.   Optional\n                Default is false.\ntext_data_num   Truncates input text to a specified length before testing.    Optional\n                If text_data_num <= 0, no truncation occurs. Default is 0.\n\n\nExecution Results#\n\n\nSimple Conversation#\n\nReference for simple dialogue testing:\n\n\n\n\nPPL Evaluation#\n\nAfter PPL evaluation completes, a {ppl_testcase}.json file will be generated in\nthe same directory. The value corresponding to Perplexity in this file is the\nfinal PPL test result. See the example below:\n\n\n\nNote\n\n 1. PPL supports resuming evaluation from checkpoints. During execution, the\n    program generates a ppl_ckpt.json file in the working directory. When\n    load_ckpt is set to true, the program reads this file and continues testing\n    from where it left off.\n\n 2. After the PPL program finishes execution, a JSON file containing key\n    parameters and the PPL calculation result of this test will be generated in\n    the directory where the bin test file resides.\n\n 3. The bin file specified by the ppl_testcase parameter can be converted from a\n    parquet file. An example reference script convert_parquet_to_bin.py is\n    provided below:\n\n","routePath":"/en/guide/quickstart/S100P/internlm2/advanced_development_internlm2","lang":"en","toc":[{"text":"Environment Setup","id":"environment-setup","depth":2,"charIndex":439},{"text":"Deployment Package Preparation","id":"deployment-package-preparation","depth":2,"charIndex":630},{"text":"Model Preparation","id":"model-preparation","depth":2,"charIndex":754},{"text":"Model Quantization","id":"model-quantization","depth":2,"charIndex":-1},{"text":"Edge Device Runtime Preparation","id":"edge-device-runtime-preparation","depth":2,"charIndex":1766},{"text":"Edge Device Execution","id":"edge-device-execution","depth":2,"charIndex":3256},{"text":"Simple Conversation","id":"simple-conversation","depth":3,"charIndex":3281},{"text":"PPL Evaluation","id":"ppl-evaluation","depth":3,"charIndex":3836},{"text":"Execution Results","id":"execution-results","depth":2,"charIndex":5331},{"text":"Simple Conversation","id":"simple-conversation-1","depth":3,"charIndex":5352},{"text":"PPL Evaluation","id":"ppl-evaluation-1","depth":3,"charIndex":5417}],"domain":"","frontmatter":{},"version":"latest"},{"id":20,"title":"Simple Development","content":"#\n\nIn this chapter, we will introduce the basic usage workflow of D-Robotics-LLM to\nhelp you get started quickly. Here, we use the InternLM2-1.8B model as an\nexample for demonstration.\n\n\nModel and Deployment Package Preparation#\n\n * Download the provided D-Robotics_LLM_.tar.gz deployment package and extract\n   it.\n\n * Download the provided InternLM2_1.8B_1024.hbm model.\n\nNote\n\nFor the download link of the hbm model, please refer to the resolve_model.txt\nfile located in the model folder of oellm_runtime.\n\nAfter preparing both the deployment package and the model, integrate the model\n(*.hbm) with the oellm_runtime SDK from the deployment package. The reference\ndirectory structure is as follows:\n\n\n\n\nOn-Device Runtime Preparation#\n\nCreate a working directory on the device using the following commands as\nreference:\n\n\n\nCopy the integrated folder from your development machine to this on-device\ndirectory. Reference command:\n\n\n\nFinally, under the path /home/root/llm/D-Robotics_LLM_{version}/oellm_runtime,\nconfigure LD_LIBRARY_PATH. Reference commands:\n\n\n\n\nOn-Device Execution#\n\nReference command to run the model on the device:\n\n\n\nThe runtime parameters are as follows:\n\nPARAMETER         DESCRIPTION                                                OPTIONAL\n-h, --help        Display help information.                                  /\n--hbm_path        Specifies the path to the quantized model file (*.hbm).    Required\n--tokenizer_dir   Specifies the tokenizer configuration directory.           Required\n--model_type      Specifies the model type to run; currently, the InternLM   Required\n                  model type is 4.\n\n\nExecution Results#\n\nAfter successful execution, you can perform a simple conversation test as shown\nbelow:\n\n","routePath":"/en/guide/quickstart/S100P/internlm2/simple_development_internlm2","lang":"en","toc":[{"text":"Model and Deployment Package Preparation","id":"model-and-deployment-package-preparation","depth":2,"charIndex":186},{"text":"On-Device Runtime Preparation","id":"on-device-runtime-preparation","depth":2,"charIndex":705},{"text":"On-Device Execution","id":"on-device-execution","depth":2,"charIndex":1062},{"text":"Execution Results","id":"execution-results","depth":2,"charIndex":1637}],"domain":"","frontmatter":{},"version":"latest"},{"id":21,"title":"Memory Allocation Instructions","content":"#\n\nThe S100 and S100P have different memory capacities, and memory usage varies\nacross different models. The base software for S100/S100P provides the\nhb_switch_ion.sh script to allocate memory space available for models. This\nscript mainly supports the following two usage modes.\n\n\nBalanced Mode#\n\n\n\nIt is recommended to use balanced mode when running models of 3B or smaller. If\nyou encounter errors while running a 7B model, please switch to bpu_first mode.\n\n\nBPU_First Mode#\n\n\n\nIn bpu_first mode, models can utilize the maximum available memory space, making\nit suitable for running 7B models.","routePath":"/en/guide/quickstart/S100P/ion_allocate","lang":"en","toc":[{"text":"Balanced Mode","id":"balanced-mode","depth":2,"charIndex":282},{"text":"BPU_First Mode","id":"bpu_first-mode","depth":2,"charIndex":462}],"domain":"","frontmatter":{},"version":"latest"},{"id":22,"title":"Qwen2.5 Model Development","content":"#\n\n\nOverview#\n\nThis section introduces the complete operational workflow of D-Robotics-LLM\nbased on the Qwen2.5 model.\n\nIt is divided into two parts—simple development and advanced\ndevelopment—covering usage procedures for different technical scenarios. You can\nchoose the appropriate approach according to your actual use case.\n\n * Simple Development: This section describes a streamlined development process\n   using the Qwen2.5-1.5B-Instruct model as an example, covering key steps from\n   model and deployment package preparation to on-device execution, helping you\n   quickly validate Qwen2.5 model development.\n\n * Advanced Development: This section presents a full-cycle development\n   framework based on the Qwen2.5 model, encompassing the entire development\n   pipeline—from model and deployment package preparation, model quantization,\n   edge-side deployment, on-device runtime setup, to on-device\n   execution—enabling you to gain a deeper understanding of the complete model\n   deployment process.\n\n\nSupported Models#\n\nD-Robotics-LLM provides quantization and edge-side deployment capabilities for\nthe Qwen2.5 model. Currently supported models include Qwen2.5-1.5B, Qwen2.5-7B,\nQwen2.5-1.5B-Instruct, and Qwen2.5-7B-Instruct.","routePath":"/en/guide/quickstart/S100P/qwen2_5","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"Supported Models","id":"supported-models","depth":2,"charIndex":1012}],"domain":"","frontmatter":{},"version":"latest"},{"id":23,"title":"Advanced Development","content":"#\n\nIn this chapter, we will introduce the advanced development workflow of\nD-Robotics-LLM.\n\nThis workflow applies to the following scenarios:\n\n 1. The model has already been fine-tuned and requires re-quantization.\n\n 2. Simple single-turn conversations.\n\n 3. Continuous multi-turn conversations, where the model can remember questions\n    and answers from previous rounds.\n\n 4. Calculating the model's PPL (Perplexity) metric on the edge device.\n\n\nQwen2.5 Model Version Selection#\n\nWe provide both Base and Instruct versions of the Qwen2.5 model to meet your\ndiverse development and application requirements. Their differences are as\nfollows:\n\n * The Base version is a foundational text generation model suitable for\n   subsequent model training tasks. Model names for this version do not contain\n   the word Instruct.\n\n * The Instruct version is derived from the Base version through instruction\n   fine-tuning and is better suited for conversational scenarios. Model names\n   for this version include the word Instruct.\n\nHere, we will use the Qwen2.5-1.5B-Instruct model as an example to illustrate\nusage instructions.\n\nPlease note specifically that continuous multi-turn conversation scenarios only\nsupport the Instruct version; all other scenarios support both the Base and\nInstruct versions.\n\n\nEnvironment Setup#\n\nPlease ensure you have correctly completed environment setup for both the\ndevelopment host and development board as described in the Environment\nDeployment section.\n\n\nDeployment Package Preparation#\n\nDownload the provided deployment package D-Robotics_LLM_{version}.tar.gz and\nextract it.\n\n\nModel Preparation#\n\nNote\n\nCurrently, only the following models are supported: Qwen2.5-1.5B, Qwen2.5-7B,\nQwen2.5-1.5B-Instruct, and Qwen2.5-7B-Instruct.\n\nBefore downloading any model, please ensure you understand its license terms,\ndependency requirements, and other necessary information to guarantee proper\nfunctionality afterward.\n\nYou can obtain Qwen2.5 series models from the Hugging Face platform using the\nlinks below:\n\n * Qwen2.5-1.5B Model\n * Qwen2.5-7B Model\n * Qwen2.5-1.5B-Instruct Model\n * Qwen2.5-7B-Instruct Model\n\n\nModel Quantization #\n\nD-Robotics-LLM provides command-line tools to quantize and compile models for\nedge-device deployment. Here, we use the Qwen2.5-1.5B-Instruct model as an\nexample. Reference command:\n\n\n\nNote\n\nFor detailed usage instructions and important considerations regarding the\noellm_build tool, please refer to the oellm_build Tool section.\n\nIf you obtain our pre-compiled .hbm models via the links provided in\nresolve_model.txt, you may skip this quantization step.\n\nAll Qwen2.5 models listed in resolve_model.txt were compiled with chunk_size set\nto 256 and cache_len set to 1024.\n\n\nEdge Device Runtime Preparation#\n\nIn the directory D-Robotics_LLM_{version}/oellm_runtime/example, we have\npre-prepared compiled executables in subdirectories that can be directly run on\nthe edge device. Alternatively, you can execute different build scripts to\ngenerate required files yourself. Reference commands:\n\n\n\nNext, create a working directory on the edge device. Reference command:\n\n\n\nBefore deploying to the board, ensure you have prepared the following:\n\n * A functional development board for executing programs on the edge device.\n * A deployable model file (*.hbm), which is the output of the Model\n   Quantization step.\n * Executable files (oellm_run, oellm_multichat, and oellm_ppl).\n * Runtime dependency libraries. To reduce deployment overhead, you can directly\n   use the contents from the following paths within the D-Robotics-LLM package:\n   * D-Robotics_LLM_{version}/oellm_runtime/set_performance_mode.sh\n   * D-Robotics_LLM_{version}/oellm_runtime/lib folder\n   * D-Robotics_LLM_{version}/oellm_runtime/config folder\n   * D-Robotics_LLM_{version}/oellm_runtime/example folder\n\nAfter preparation, integrate the model files (*.hbm), executables, and\ndependency libraries together. Reference directory structure:\n\n\n\n\nEdge Device Runtime Setup#\n\nCreate a working directory on the edge device. Reference commands:\n\n\n\nCopy the integrated folder from your development host to this edge device\ndirectory. Reference command:\n\n\n\nFinally, configure LD_LIBRARY_PATH under the path\n/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime. Reference commands:\n\n\n\n\nRunning on Edge Device#\n\n\nSimple Conversation#\n\nReference command for running on the edge device:\n\n\n\nInput parameters for the program:\n\nPARAMETER         DESCRIPTION                                                OPTIONAL/REQUIRED\n-h, --help        Display help information.                                  /\n--hbm_path        Specifies the path to the quantized model file (*.hbm).    Required\n--tokenizer_dir   Specifies the tokenizer configuration directory.           Required\n--template_path   Specifies the conversation template path for Instruct      Optional\n                  models; omit when loading Base models.\n--model_type      Specifies the model type to run; currently, Qwen2.5 uses   Required\n                  model type 7.\n\n\nMulti-turn Conversation#\n\nReference command for running on the edge device:\n\n\n\nInput parameters for the program:\n\n| Parameter | Description | Optional/Required || -------- | ------------- |\n------------ | | -h, --help | Display help information. | / | | -c, --config |\nSpecify the path to the JSON configuration file. | Required |\n\nExample JSON configuration file:\n\n\n\nParameter descriptions for the JSON configuration file:\n\nPARAMETER       DESCRIPTION                                                OPTIONAL/REQUIRED\nhbm_path        Path to the quantized model file (*.hbm).                  Required\ntokenizer_dir   Path to the tokenizer configuration directory.             Required\ntemplate_path   Path to the conversation template file.                    Required\nmodel_type      Specifies the model type to run; currently, Qwen2.5 uses   Required\n                model type 7.\nbpu_core        Specifies the BPU core to use. Default is -1 (any core).   Optional\n\n\nPPL Evaluation#\n\nTo evaluate the model's Perplexity (PPL) on-device, refer to the following\ncommands:\n\n\n\nProgram input parameters:\n\nPARAMETER      DESCRIPTION                                        OPTIONAL/REQUIRED\n-h, --help     Display help information.                          /\n-c, --config   Specify the path to the JSON configuration file.   Required\n\nExample JSON configuration file:\n\n\n\nParameter descriptions for the JSON configuration file:\n\nPARAMETER       DESCRIPTION                                                OPTIONAL/REQUIRED\nhbm_path        Path to the quantized model file (*.hbm).                  Required\ntokenizer_dir   Path to the tokenizer configuration directory.             Required\nmodel_type      Specifies the model type to run; currently, Qwen2.5 uses   Required\n                model type 7.\nppl_testcase    Path to the test file; currently only bin format is        Required\n                supported.\nmax_length      Sequence length fed into the model per evaluation step.    Required\nstride          Stride used during testing.                                Required\nbpu_core        Specifies the BPU core to use. Default is -1 (any core).   Optional\nload_ckpt       Whether to resume testing from the last checkpoint if      Optional\n                interrupted. Default is false.\ntext_data_num   Truncates the input text to a specific length before       Optional\n                testing. If text_data_num <= 0, no truncation occurs.\n                Default is 0.\n\n\nExecution Results#\n\n\nSimple Conversation#\n\nExample of a simple conversation test:\n\n\n\n\nMulti-turn Conversation#\n\nExample of a multi-turn conversation test:\n\n\n\n\nPPL Evaluation#\n\nAfter PPL evaluation completes, a {ppl_testcase}.json file is generated in the\nsame directory. The value under Perplexity represents the final PPL result.\nExample:\n\n\n\nNote\n\n 1. PPL evaluation supports resuming from checkpoints. During execution, a\n    ppl_ckpt.json file is created in the working directory. If load_ckpt is set\n    to true, the program resumes testing from this checkpoint.\n\n 2. After PPL evaluation finishes, a JSON file containing key test parameters\n    and the PPL result is generated in the directory of the bin test file.\n\n 3. The bin file specified by ppl_testcase can be converted from a parquet file.\n    Below is reference code (convert_parquet_to_bin.py) for conversion:\n\n","routePath":"/en/guide/quickstart/S100P/qwen2_5/advanced_development_qwen","lang":"en","toc":[{"text":"Qwen2.5 Model Version Selection","id":"qwen25-model-version-selection","depth":2,"charIndex":447},{"text":"Environment Setup","id":"environment-setup","depth":2,"charIndex":1298},{"text":"Deployment Package Preparation","id":"deployment-package-preparation","depth":2,"charIndex":1485},{"text":"Model Preparation","id":"model-preparation","depth":2,"charIndex":1609},{"text":"Model Quantization","id":"model-quantization","depth":2,"charIndex":-1},{"text":"Edge Device Runtime Preparation","id":"edge-device-runtime-preparation","depth":2,"charIndex":2734},{"text":"Edge Device Runtime Setup","id":"edge-device-runtime-setup","depth":2,"charIndex":3972},{"text":"Running on Edge Device","id":"running-on-edge-device","depth":2,"charIndex":4306},{"text":"Simple Conversation","id":"simple-conversation","depth":3,"charIndex":4332},{"text":"Multi-turn Conversation","id":"multi-turn-conversation","depth":3,"charIndex":5051},{"text":"PPL Evaluation","id":"ppl-evaluation","depth":3,"charIndex":6021},{"text":"Execution Results","id":"execution-results","depth":2,"charIndex":7529},{"text":"Simple Conversation","id":"simple-conversation-1","depth":3,"charIndex":7550},{"text":"Multi-turn Conversation","id":"multi-turn-conversation-1","depth":3,"charIndex":7615},{"text":"PPL Evaluation","id":"ppl-evaluation-1","depth":3,"charIndex":7688}],"domain":"","frontmatter":{},"version":"latest"},{"id":24,"title":"Simple Development","content":"#\n\nIn this chapter, we will introduce the basic usage workflow of D-Robotics-LLM to\nhelp you get started quickly.\n\n\nQwen2.5 Model Version Selection#\n\nWe provide both Base and Instruct versions of the Qwen2.5 model to meet your\ndiverse development and application needs. The differences between them are as\nfollows:\n\n * The Base version is a foundational text generation model suitable for\n   subsequent model training tasks. Its model name does not contain the word\n   Instruct.\n\n * The Instruct version is derived from the Base version through instruction\n   fine-tuning and is better suited for conversational scenarios. Its model name\n   does contain the word Instruct.\n\nHere, we use the Qwen2.5-1.5B-Instruct model as an example to demonstrate its\nusage.\n\n\nPreparing the Model and Deployment Package#\n\n * Download and extract the provided deployment package: D-Robotics_LLM_.tar.gz.\n\n * Download the provided model file: Qwen2.5_1.5B_Instruct_1024.hbm.\n\nNote\n\nFor the download link of the .hbm model, please refer to the resolve_model.txt\nfile located in the model folder of oellm_runtime.\n\nAfter preparing both the deployment package and the model, integrate the model\n(*.hbm) with the oellm_runtime SDK from the deployment package. The reference\ndirectory structure is as follows:\n\n\n\n\nOn-Device Setup#\n\nCreate a working directory on the device using the following commands as a\nreference:\n\n\n\nCopy the integrated folder from your development machine to this on-device\ndirectory. Reference command:\n\n\n\nFinally, under the path /home/root/llm/D-Robotics_LLM_{version}/oellm_runtime,\nconfigure the LD_LIBRARY_PATH. Reference commands:\n\n\n\n\nRunning on Device#\n\nReference command to run the model on the device:\n\n\n\nProgram input parameters are as follows:\n\nPARAMETER         DESCRIPTION                                               OPTIONAL/REQUIRED\n-h, --help        Display help information.                                 /\n--hbm_path        Specifies the path to the quantized model file (*.hbm).   Required\n--tokenizer_dir   Specifies the tokenizer configuration directory.          Required\n--template_path   Specifies the conversation template path for Instruct     Optional\n                  models; optional when loading Base models.\n--model_type      Specifies the model type to run; currently, the Qwen2.5   Required\n                  model type is 7.\n\n\nRunning Results#\n\nAfter successful execution, you can perform simple conversation tests, as shown\nbelow:\n\n","routePath":"/en/guide/quickstart/S100P/qwen2_5/simple_development_qwen","lang":"en","toc":[{"text":"Qwen2.5 Model Version Selection","id":"qwen25-model-version-selection","depth":2,"charIndex":115},{"text":"Preparing the Model and Deployment Package","id":"preparing-the-model-and-deployment-package","depth":2,"charIndex":760},{"text":"On-Device Setup","id":"on-device-setup","depth":2,"charIndex":1290},{"text":"Running on Device","id":"running-on-device","depth":2,"charIndex":1639},{"text":"Running Results","id":"running-results","depth":2,"charIndex":2364}],"domain":"","frontmatter":{},"version":"latest"},{"id":25,"title":"Qwen2.5-Omni Model Development","content":"#\n\n\nOverview#\n\nThis section introduces the end-to-end operational framework of D-Robotics-LLM\nbased on the Qwen2.5-Omni model.\n\nIt is divided into two parts—simple development and advanced\ndevelopment—covering usage workflows for different technical scenarios. You can\nchoose the approach that best fits your actual use case.\n\n * Simple Development: This section describes a streamlined development workflow\n   using the Qwen2.5-Omni-3B model as an example, covering key steps from model\n   and deployment package preparation to on-device execution, enabling you to\n   quickly validate Qwen2.5-Omni model development.\n\n * Advanced Development: This section presents a full-cycle development pipeline\n   based on the Qwen2.5-Omni model, encompassing the entire workflow—from model\n   and deployment package preparation, model quantization, edge-side deployment,\n   on-device runtime setup, to final on-device execution—to help you gain a\n   deeper understanding of the complete model deployment process.\n\n\nSupported Models#\n\nD-Robotics-LLM provides quantization and edge-side deployment capabilities for\nthe Qwen2.5-Omni model. The currently supported model includes Qwen2.5-Omni-3B.","routePath":"/en/guide/quickstart/S100P/qwen2_5_omni","lang":"en","toc":[{"text":"Overview","id":"overview","depth":2,"charIndex":3},{"text":"Supported Models","id":"supported-models","depth":2,"charIndex":1004}],"domain":"","frontmatter":{},"version":"latest"},{"id":26,"title":"Advanced Development","content":"#\n\nIn this chapter, we will introduce the advanced development workflow of\nD-Robotics-LLM.\n\nThis workflow applies to the following scenarios:\n\n 1. Quantizing models yourself.\n\n 2. Offline execution: The model generates textual responses by reading local\n    audio, video, or image data.\n\n 3. Online execution: The model generates textual responses by streaming audio\n    or video data. Compared to offline execution, online execution processes\n    data while it is being transmitted, significantly reducing the latency\n    before the model outputs its first token.\n\nFor the above scenarios, we will continue using the Qwen2.5_Omni_3B model as an\nexample to demonstrate usage.\n\n\nEnvironment Setup#\n\nPlease ensure you have correctly completed environment setup for both the\ndevelopment host and development board as described in the Environment\nDeployment section.\n\n\nDeployment Package Preparation#\n\nDownload the provided deployment package D-Robotics_LLM_{version}.tar.gz and\nextract it.\n\n\nModel Preparation#\n\nNote\n\nCurrently, only the Qwen2.5-Omni-3B model is supported. Before downloading the\nmodel, please ensure you understand the model's license terms, required\ndependencies, and other necessary information to guarantee proper subsequent\nusage.\n\nYou can obtain Omni-series models from the Hugging Face platform. Below is the\ndownload link for the model:\n\n * Qwen/Qwen2.5-Omni-3B Model\n\n\nModel Quantization #\n\nD-Robotics-LLM provides a command-line tool to quantize and compile models for\non-device deployment. Using the Qwen2.5-Omni-3B model as an example, the\nreference command is as follows:\n\n\n\nNote\n\nFor detailed usage instructions and important considerations regarding the\noellm_build tool, please refer to the oellm_build Tool section.\n\nIf you obtain our pre-compiled HBM models via the links provided in\nresolve_model.txt, you may skip this model quantization step.\n\nAll Omni models provided in the resolve_model.txt file are compiled with\nchunk_size set to 256 and cache_len configured to 2048. Currently, only this\nconfiguration is supported.\n\n\nMultimodal Support #\n\nThe Qwen2.5_Omni_3B model supports multiple modalities including text, audio,\nimages, and video. Regardless of input combinations, the model always outputs\nplain text.\n\nMultimodal support operates in two modes—offline and online—with slight\ndifferences in supported input combinations, as detailed below:\n\n\nOffline Execution#\n\nNO.   TEXT   AUDIO   IMAGE   VIDEO\n1     Y      N/A     N/A     N/A\n2     N/A    Y       N/A     N/A\n3     N/A    N/A     Y       N/A\n4     N/A    N/A     N/A     Y\n5     Y      N/A     Y       N/A\n6     N/A    Y       Y       N/A\n7     Y      N/A     N/A     Y\n8     N/A    Y       N/A     Y\n\n * Text content should be included directly within the JSON file; no separate\n   text file is needed.\n\n * Supported audio formats include mp3, wav, and flac, with a maximum duration\n   of 30 seconds.\n\n * Supported image formats include jpg, png, bmp, and jpeg; images will be\n   resized to a fixed resolution of 448x448.\n\n * Supported video formats include mp4 and mkv, with a maximum duration of 5\n   seconds. Videos are sampled at 2 frames per second and resized to 448x448.\n   Additionally, if a video contains audio and no separate audio input is\n   provided, the embedded audio will be processed. If a separate audio input is\n   provided, the audio embedded in the video will be ignored.\n\nAll modal inputs must be configured via a JSON file. For detailed instructions,\nplease refer to the On-Device Execution section.\n\n\nOnline Execution#\n\nNO.   TEXT   AUDIO   VIDEO\n1     Y      N/A     Y\n2     N/A    Y       Y\n3     N/A    Y       N/A\n\n * Text content can be fed to the model using the xlm_omni_feed_text_online API.\n\n * Video format is limited to nv12. You can use the xlm_omni_feed_video_online\n   API to feed single-frame nv12 data to the model. Frames will be resized to\n   448x448, and each conversation supports transmission of 2 to 10 frames.\n\n * Audio data must be of type float32 with values in the range [-1, 1]. You can\n   use the xlm_omni_feed_audio_online API to either transmit complete audio in\n   one go or stream audio segments incrementally. Each conversation supports up\n   to 30 seconds of cumulative audio.\n\n\nOn-Device Execution Preparation#\n\nWithin the directory D-Robotics_LLM_{version}/oellm_runtime/example, we have\npre-prepared compiled executables in subdirectories that can be run directly on\nthe device. Alternatively, you can generate the required files yourself by\nexecuting different build scripts. Reference commands are as follows:\n\n\n\nNext, create a working directory on the device with the following command:\n\n\n\nBefore execution, ensure the following items are ready:\n\n * A functional development board for running on-device programs.\n * On-device deployable model files (*.hbm).\n * Input embedding weights (embed_tokens.bin).\n * Executable files (oellm_omni_offline and oellm_omni_online) along with their\n   corresponding JSON configuration files.\n * Required runtime libraries. To simplify deployment, you may directly use the\n   contents from the following directories within the D-Robotics-LLM package:\n   * D-Robotics_LLM_{version}/oellm_runtime/set_performance_mode.sh\n   * D-Robotics_LLM_{version}/oellm_runtime/lib\n   * D-Robotics_LLM_{version}/oellm_runtime/config\n   * D-Robotics_LLM_{version}/oellm_runtime/example\n\nAfter preparing all necessary files, organize them into a unified directory\nstructure as shown below:\n\n\n\nCopy the prepared folder from your development host to the device directory\nusing the following command:\n\n\n\nFinally, configure LD_LIBRARY_PATH under the path\n/home/root/llm/D-Robotics_LLM_{version}/oellm_runtime with the following\ncommands:\n\n\n\n\nOn-Device Execution #\n\n\nOffline Execution#\n\nReference command for offline execution:\n\n\n\nProgram arguments are as follows:\n\nARGUMENT       DESCRIPTION                                               REQUIRED\n-h, --help     Display help information.                                 /\n-c, --config   Specify the path to the JSON configuration file used at   Required\n               runtime.\n\nExample JSON configuration file:\n\n\n\nThe parameters in the JSON configuration file are described as follows:\n\nPARAMETER         DESCRIPTION                                                    OPTIONAL/REQUIRED\nvisual_hbm_path   Specifies the path to the quantized video/image feature        Required\n                  extraction model file (*.hbm).\naudio_hbm_path    Specifies the path to the quantized audio feature extraction   Required\n                  model file (*.hbm).\ntext_hbm_path     Specifies the path to the quantized text model file (*.hbm).   Required\nembed_tokens      Specifies the path to the model's input embedding weights      Required\n                  (embed_tokens.bin).\ntokenizer_dir     Specifies the path to the tokenizer and partial                Required\n                  initialization data configuration.\nmodel_type        Specifies the model type to run; the current Omni model type   Required\n                  is 5.\nonline_mode       Specifies whether the model runs in online or offline          Required\n                  mode.Valid values: 'true', 'false'.\n\nWhen running the program, you also need to provide the path to a JSON file\ncontaining multimodal input information via the command line, then press Enter\nto start the interaction.\n\nOffline execution supports modalities including text, audio, images, and video.\nYou must prepare the input information in advance within the JSON file and save\nit locally. The template is as follows:\n\nNote\n\nNote: This JSON file template is provided for illustrative purposes only. For\ndetails on supported multimodal input combinations, please refer to the\nMultimodal Support section.\n\n\n\nIn this JSON configuration template, the same conversation node includes the\nsystem role with a text field, and the user role with optional text, audio,\nimage, and video fields. If a particular modality is not needed, you must delete\nthe entire corresponding object (including its braces).\n\nFor example, when providing only video input, the JSON file can be configured as\nfollows:\n\n\n\n\nOnline Execution#\n\nThe D-Robotics-LLM deployment package provides an API that supports\nstreaming-based online execution of the Qwen2.5_Omni_3B model. We provide an\nonline execution example for reference.\n\nThe reference command to run this example on the device is as follows:\n\n\n\nThe program accepts the following command-line arguments:\n\nPARAMETER      DESCRIPTION                                                  OPTIONAL/REQUIRED\n-h, --help     Display help information.                                    /\n-c, --config   Specifies the path to the runtime JSON configuration file.   Required\n\nAn example JSON configuration file is shown below:\n\n\n\nThe parameters in the JSON configuration file are described as follows:\n\nPARAMETER         DESCRIPTION                                                    OPTIONAL/REQUIRED\nvisual_hbm_path   Specifies the path to the quantized video/image feature        Required\n                  extraction model file (*.hbm).\naudio_hbm_path    Specifies the path to the quantized audio feature extraction   Required\n                  model file (*.hbm).\ntext_hbm_path     Specifies the path to the quantized text model file (*.hbm).   Required\nembed_tokens      Specifies the path to the model's input embedding weights      Required\n                  (embed_tokens.bin).\ntokenizer_dir     Specifies the path to the tokenizer and partial                Required\n                  initialization data configuration.\nmodel_type        Specifies the model type to run; the current Omni model type   Required\n                  is 5.\nonline_mode       Specifies whether the model runs in online or offline          Required\n                  mode.Valid values: 'true', 'false'.\nvideo_path        Specifies the path to the video file to be processed during    Required\n                  online execution.\nuser_text         Specifies the user's textual input content.                    Optional\n\nTip\n\nAudio data in online execution is extracted from the video. If you test with\nyour own video data, we recommend using videos that contain audio tracks.\n\n\nExecution Results#\n\n\nOffline Execution#\n\nA complete offline interaction proceeds as follows. [User] <<< is followed by\nthe path to your JSON file containing the input data (e.g.,\n./omni_offline_prompt.json), and [Assistant] >>> shows the model’s textual\noutput. Before generating the response, the program prints the input information\nfrom the JSON file to the terminal.\n\n\n\n\nOnline Execution#\n\nEnter 1, 2, or 3 to run one of the three supported multimodal input combinations\nin online mode; enter 0 to exit the program. A full demonstration of the online\ninteraction is shown below:\n\n","routePath":"/en/guide/quickstart/S100P/qwen2_5_omni/advanced_development_omni","lang":"en","toc":[{"text":"Environment Setup","id":"environment-setup","depth":2,"charIndex":677},{"text":"Deployment Package Preparation","id":"deployment-package-preparation","depth":2,"charIndex":864},{"text":"Model Preparation","id":"model-preparation","depth":2,"charIndex":988},{"text":"Model Quantization","id":"model-quantization","depth":2,"charIndex":-1},{"text":"Multimodal Support","id":"multimodal-support","depth":2,"charIndex":-1},{"text":"Offline Execution","id":"offline-execution","depth":3,"charIndex":2387},{"text":"Online Execution","id":"online-execution","depth":3,"charIndex":3526},{"text":"On-Device Execution Preparation","id":"on-device-execution-preparation","depth":2,"charIndex":4238},{"text":"On-Device Execution","id":"on-device-execution","depth":2,"charIndex":-1},{"text":"Offline Execution","id":"offline-execution-1","depth":3,"charIndex":5745},{"text":"Online Execution","id":"online-execution-1","depth":3,"charIndex":8157},{"text":"Execution Results","id":"execution-results","depth":2,"charIndex":10240},{"text":"Offline Execution","id":"offline-execution-2","depth":3,"charIndex":10261},{"text":"Online Execution","id":"online-execution-2","depth":3,"charIndex":10615}],"domain":"","frontmatter":{},"version":"latest"},{"id":27,"title":"Simple Development","content":"#\n\nIn this chapter, we will introduce the basic usage workflow of D-Robotics-LLM to\nhelp you get started quickly. Here, we take the Qwen2.5-Omni-3B model as an\nexample to demonstrate its usage.\n\n\nModel and Deployment Package Preparation#\n\n * Download the provided D-Robotics_LLM_.tar.gz deployment package and extract\n   it.\n\n * Download the provided models: Qwen2.5_Omni_3B_Audio.hbm,\n   Qwen2.5_Omni_3B_Visual.hbm, Qwen2.5_Omni_3B_Text.hbm, as well as the input\n   embedding weight file embed_tokens.bin.\n\nNote\n\nFor download links to the hbm models, please refer to the resolve_model.txt file\nlocated in the model folder of oellm_runtime.\n\nAfter preparation, integrate the models (*.hbm), the embed_tokens.bin file, and\nthe oellm_runtime SDK from the deployment package. The reference directory\nstructure is as follows:\n\n\n\n\nOn-Device Runtime Preparation#\n\nCreate a working directory on the device. Reference commands are as follows:\n\n\n\nCopy the integrated folder from your development machine to this on-device\ndirectory. Reference command:\n\n\n\nFinally, under the path /home/root/llm/D-Robotics_LLM_{version}/oellm_runtime,\nconfigure the LD_LIBRARY_PATH. Reference commands:\n\n\n\n\nOn-Device Execution#\n\nThe Qwen2.5_Omni_3B model supports both online and offline execution modes.\nTaking offline mode as an example, the reference command to run the model on the\ndevice is as follows:\n\n\n\nIn offline mode, the executable accepts the following arguments:\n\nARGUMENT       DESCRIPTION                                                    OPTIONAL/REQUIRED\n-h, --help     Display help information.                                      /\n-c, --config   Specify the path to the JSON configuration file for runtime.   Required\n\nExample JSON configuration file:\n\n\n\nParameters in the JSON configuration file are described below:\n\nPARAMETER         DESCRIPTION                                                    OPTIONAL/REQUIRED\nvisual_hbm_path   Path to the quantized video/image feature extraction model     Required\n                  file (*.hbm).\naudio_hbm_path    Path to the quantized audio feature extraction model file      Required\n                  (*.hbm).\ntext_hbm_path     Path to the quantized text model file (*.hbm).                 Required\nembed_tokens      Path to the model's input embedding weights                    Required\n                  (embed_tokens.bin).\ntokenizer_dir     Path to the tokenizer and related initialization data          Required\n                  configuration.\nmodel_type        Specifies the model type to run; the current Omni model type   Required\n                  is 5.\nonline_mode       Specifies whether the model runs in online or offline          Required\n                  mode.Valid values: 'true', 'false'.\n\n\nExecution Results#\n\nAfter running, you can perform testing as shown below:\n\n\n\nWhen running the Qwen2.5_Omni_3B model, you need to provide the path to a JSON\nfile, in which you configure inputs such as audio, video, images, and text.\nDuring execution, the program prints the input information from the JSON file to\nthe terminal.\n\nIn this example, the content of omni_offline_prompt.json is as follows:\n\n\n\nFor comprehensive details on supported multimodal inputs, specific instructions\nfor online execution mode, and detailed specifications for filling out the JSON\nfiles, please refer to the Advanced Development section.","routePath":"/en/guide/quickstart/S100P/qwen2_5_omni/simple_development_omni","lang":"en","toc":[{"text":"Model and Deployment Package Preparation","id":"model-and-deployment-package-preparation","depth":2,"charIndex":195},{"text":"On-Device Runtime Preparation","id":"on-device-runtime-preparation","depth":2,"charIndex":825},{"text":"On-Device Execution","id":"on-device-execution","depth":2,"charIndex":1179},{"text":"Execution Results","id":"execution-results","depth":2,"charIndex":2752}],"domain":"","frontmatter":{},"version":"latest"},{"id":28,"title":"The oellm_build Tool","content":"#\n\nThe oellm_build tool is provided by D-Robotics to convert floating-point model\ninto quantized model. It completes model quantization and compilation based on\nthe original floating-point model, a json configuration file(optional), and\ncalibration data(optional), and finally generates a deployable *.hbm model.\n\n\nUsage#\n\n\n\n\nParameters Introduction#\n\n\nJSON Configuration File Description #\n\n 1. The JSON configuration file for text calibration data, with a sample shown\n    below:\n    \n    \n\n 2. Calibrate data required for the Qwen2.5-Omni model , with a sample shown\n    below:\n    \n    \n\nDescription of configuration file parameters:\n\n(1) When \"role\" is \"system\", the first element in the content list must be a\ntext element containing the text field; otherwise, an error will occur when\naccessing text during template formatting.\n\nAnnotation\n\nMultiple system messages are supported within the same conversation, but the\nfirst message must be a text element containing the text field. Other system\nmessages (beyond the first) can be of text,audio,image or video.\n\n(2) When \"role\" is \"user\", the content list supports types including\ntext,audio,image,video.The specific rules are as follows:\n\nAnnotation\n\nThecontent list supports two message organization formats:\n\n 1. Messages of the same type: Can include a single or multiple items(e.g.,\n    multiple text messages, multiple imagemessages, multiple videomessages,\n    multiple audiomessages).\n\n 2. Messages of different types: Multiple types can be combined(e.g., a\n    combination of text+image+audio messages).\n\n * When the type in the content list is \"text\":\n   \n   * Format restrictions:No special format requirements; plain text, sentences\n     with punctuation, short instructions, and long paragraphs are all\n     supported.\n   \n   * Supported sources:No fixed source restrictions.\n   \n   * Example reference:\n   \n   \n\n * When the type in the content list is \"video\":\n   \n   * Format restrictions:MP4、MKV.\n   \n   * Supported sources:Local video files, local file URLs, and web URLs.\n   \n   * Example reference:\n   \n   \n\n * When the type in the content list is \"image\"`:\n   \n   * Format restrictions:PNG、JPG、JPEG、BMP.\n   \n   * Supported sources:Local image files, local file URLs, web URLs, and Data\n     URI.\n   \n   * Example reference:\n   \n   \n\n * When the type in the content list is \"audio\":\n   \n   * Format restrictions:WAV、MP3、FLAC.\n   \n   * Supported sources:Local audio files, local file URLs, web URLs, Data URI.\n   \n   * Example reference:\n   \n   \n\n\nUsage Example#\n\n * The DeepSeek-R1-Distill-Qwen model uses the oellm_build tool for model\n   quantization. Refer to the following command:\n   \n   \n\n * The DeepSeek-R1-Distill-Qwen model uses the oellm_build tool for model\n   quantization and perform consistency verification on the quantized HBM model.\n   Refer to the following command:\n   \n   \n\n * The InternLM2 model uses the oellm_build tool to perform model quantization.\n   Refer to the following command:\n   \n   \n\n * The Qwen2.5 model uses the oellm_build tool to perform model quantization.\n   Refer to the following command:\n   \n   \n\n * The Qwen2.5-Omni model uses the oellm_build tool for model quantization.\n   Refer to the following command:\n   \n   ","routePath":"/en/guide/tool_introduction/oellm_build","lang":"en","toc":[{"text":"Usage","id":"usage","depth":2,"charIndex":314},{"text":"Parameters Introduction","id":"parameters-introduction","depth":2,"charIndex":325},{"text":"JSON Configuration File Description","id":"json-configuration-file-description","depth":2,"charIndex":-1},{"text":"Usage Example","id":"usage-example","depth":2,"charIndex":2521}],"domain":"","frontmatter":{},"version":"latest"}]